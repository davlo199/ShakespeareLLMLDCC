{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZ-AwbZ4ySCJ"
   },
   "source": [
    "## Part 2 code for LLM performance evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_5IXGh6OOBZV"
   },
   "outputs": [],
   "source": [
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "torch.manual_seed(305)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "A_Z5Jh74DH_E"
   },
   "outputs": [],
   "source": [
    "# Global hyperparameters\n",
    "SMALL_ITERS = 1000\n",
    "LARGE_ITERS = 2000\n",
    "EVAL_ITERS = 100\n",
    "CONTEXT_WINDOW_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "53dGz7ExDkUv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 1,115,394\n"
     ]
    }
   ],
   "source": [
    "# download the tiny shakespeare dataset\n",
    "input_file_path = 'input.txt'\n",
    "\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "print(f\"length of dataset in characters: {len(data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 796,570 tokens\n",
      "val has 89,145 tokens\n",
      "Dictionary has 115 tokens\n"
     ]
    }
   ],
   "source": [
    "# find the most common bigrams and trigrams (doubles and triple character combination)\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Ensure 'data' is defined with your full text\n",
    "n = len(data)\n",
    "train_chars = data[:int(n * 0.9)]\n",
    "val_chars = data[int(n * 0.9):]\n",
    "\n",
    "def BiandTrigrams(text, top=10):\n",
    "    \n",
    "    text = text.replace(\"\\n\", \"\").replace(\" \", \"\")  # Remove spaces and newlines\n",
    "    \n",
    "    ngram_count = Counter()\n",
    "    #pdb.st_trace()\n",
    "    for size in [2,3]:\n",
    "        ngrams = [text[i:i+size] for i in range(len(text) - size + 1)]\n",
    "        ngram_count.update(ngrams)\n",
    "    \n",
    "    return [ngram for ngram, _ in ngram_count.most_common(top)]\n",
    "\n",
    "\n",
    "n_biandtri = 50\n",
    "\n",
    "out = BiandTrigrams(train_chars, n_biandtri)\n",
    "#top 100 bigrams and trigrams\n",
    "\n",
    "# get all the unique characters that occur in this text\n",
    "chars = sorted(list(set(data)))\n",
    "\n",
    "dic = chars+list(out)\n",
    "\n",
    "# create a mapping from dic tionary of integers and bi/trigrams to integers\n",
    "stoi2 = { ch:i for i,ch in enumerate(dic) }\n",
    "itos2 = { i:ch for i,ch in enumerate(dic) }\n",
    "\n",
    "def encodeBiandTri(text):\n",
    "    \n",
    "    i = 0\n",
    "    encoded_string = []\n",
    "    while i < len(text):\n",
    "\n",
    "        if i + 2 < len(text) and text[i:i+3] in stoi2:\n",
    "            encoded_string.append(stoi2[text[i:i+3]])\n",
    "            i += 3 #checks next three integers\n",
    "\n",
    "        elif i + 1 < len(text) and text[i:i+2] in stoi2:\n",
    "            encoded_string.append(stoi2[text[i:i+2]])\n",
    "            i += 2\n",
    "        # As above with doubles\n",
    "        elif text[i] in stoi2:\n",
    "            encoded_string.append(stoi2[text[i]])\n",
    "            i += 1\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown token at index {i}: {text[i]}\")\n",
    "    return encoded_string\n",
    "\n",
    "def decode2(l):\n",
    "    return ''.join([itos2[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "\n",
    "# Encode both datasets into lists of integers\n",
    "train_data = encodeBiandTri(train_chars)\n",
    "val_data = encodeBiandTri(val_chars)\n",
    "\n",
    "# Cast the lists to torch tensors\n",
    "train_data = torch.tensor(train_data)\n",
    "val_data = torch.tensor(val_data)\n",
    "\n",
    "print(f\"train has {len(train_data):,} tokens\")\n",
    "print(f\"val has {len(val_data):,} tokens\")\n",
    "\n",
    "vocab_size=len(dic)\n",
    "print(f\"Dictionary has {vocab_size} tokens\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load GPT2-XL\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')\n",
    "model = GPT2Model.from_pretrained('gpt2-xl')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clear MPS\n",
    "import gc\n",
    "torch.mps.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize under gpt2-xl\n",
    "from transformers import GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-xl\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-xl\", torch_dtype=torch.float32)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, context_window_size, embed_size=384):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          head_size: int, size of the head embedding dimension (K)\n",
    "          context_window_size: int, number of tokens considered in the past for attention (T)\n",
    "          embed_size: int, size of the token embedding dimension (D)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_size, embed_size, bias=False)\n",
    "\n",
    "        # not a param of the model, so registered as a buffer\n",
    "        self.register_buffer('tril', torch.tril(\n",
    "            torch.ones(context_window_size, context_window_size))) # lower diagonal matrix\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x: (B,T,D) tensor of token embeddings\n",
    "\n",
    "        Returns:\n",
    "          (B,T,D) tensor of attention-weighted token embeddings\n",
    "        \"\"\"\n",
    "        # attn_weights = torch.einsum('btd, btd -> btt', self.query(x), self.key(x)) # (B,T)\n",
    "        # attn_weights = self.query(x) @ self.key(x).transpose(1,2) # (B,T,T)\n",
    "\n",
    "        attn_weights = self.query(x) @ self.key(x).transpose(-2,-1) # (B,T,T)\n",
    "        # (self.query(x) @ self.key(x).transpose(1,2)).shape # (B,T,T)\n",
    "        # if len(attn_weights.shape) == 2:\n",
    "        #     attn_weights = attn_weights.unsqueeze(0)\n",
    "        attn_weights = attn_weights.masked_fill((self.tril==0)[0:attn_weights.shape[1],0:attn_weights.shape[2]], float('-inf')) # (B,T,T)\n",
    "        attn_weights = F.softmax(attn_weights/(self.head_size**(1/2)), dim=-1) # (B,T,T)\n",
    "        # if len(attn_weights.shape) == 2:\n",
    "        #     avg_embeddings = torch.einsum('tt, td -> td', attn_weights, self.value(x)) #\n",
    "        # else:\n",
    "\n",
    "        # avg_embeddings = torch.einsum('btt, btd -> btd', attn_weights, self.value(x)) # (B,T,D) # self.value(x) gives (B,T,D), attn_weights gives (B,T)\n",
    "        avg_embeddings = torch.einsum('bij,bjd->bid', attn_weights, self.value(x))\n",
    "        return avg_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base model for non-variable attention\n",
    "\n",
    "\n",
    "class MultiHeadedAttentionLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6):\n",
    "      super().__init__()\n",
    "      self.head_size = embed_size // num_heads\n",
    "      self.context_window_size = context_window_size\n",
    "      # TODO: your code below\n",
    "      self.token_embedding_table = nn.Embedding(vocab_size, embed_size) # X which we will pass to the head\n",
    "      self.position_embedding_table = nn.Embedding(context_window_size, embed_size)\n",
    "      self.atten_head = MultiHeadAttention(context_window_size, num_heads, self.head_size, embed_size)\n",
    "      self.lm_head = nn.Linear(embed_size, vocab_size)\n",
    "      self.vocab_size = vocab_size\n",
    "      self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, token_ids, targets=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          token_ids: (B, T) token ids that make up the context (batch has size B, each entry in the\n",
    "                     batch has length T)\n",
    "          targets: (B, T) token ids corresponding to the target of each context in token_ids\n",
    "\n",
    "        Returns:\n",
    "          logits: (B, T, V), logits[b,t] gives the length V vector of logits for the next token\n",
    "                  prediction in string b up to t tokens\n",
    "          loss: scalar, negative log likelihood of target given context\n",
    "        \"\"\"\n",
    "        # TODO: your code below\n",
    "\n",
    "        B, T = token_ids.shape # (batch size, length)\n",
    "        tok_emb = self.token_embedding_table(token_ids) # (B,T,D)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,D)\n",
    "        x = tok_emb + pos_emb # (B,T,D)\n",
    "        x = self.atten_head(x) # (B,T,D)\n",
    "        logits = self.lm_head(x) # (B,T,V)\n",
    "        loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fcn(logits.reshape(-1, self.vocab_size), targets.reshape(-1)) # as before\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, token_ids, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          token_ids: (B, T) tensor of token ids to provide as context\n",
    "          max_new_tokens: int, maximum number of new tokens to generate\n",
    "\n",
    "        Returns:\n",
    "          (B, T+max_new_tokens) tensor of context with new tokens appended\n",
    "        \"\"\"\n",
    "        # TODO: your code below\n",
    "        pass\n",
    "\n",
    "class TransformerBlock_2(nn.Module):\n",
    "    \"\"\" Transformer block: communication across sequence length, followed by communication across embedding space\n",
    "        Uses multi-headed attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        # TODO: your code below\n",
    "        self.feed_forward = FeedForward(embed_size) # acts along rows x_t^(m+1) = mlp(y_t^(m))\n",
    "        self.atten_heads = MultiHeadAttention(context_window_size, num_heads, embed_size//num_heads, embed_size) # as before\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        # for dropout\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) # takes a dropout rate arg\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.atten_heads(self.ln1(x)) # communication over sequence length y_t^(m) = atten_heads(x_t^(m))\n",
    "        x = x + self.feed_forward(self.ln2(x)) # communication across embedding space x_t^(m+1) = mlp(y_t^(m))\n",
    "\n",
    "\n",
    "\n",
    "        # for adding dropout\n",
    "        # x = x + self.dropout(self.atten_heads(self.ln1(x))) # communication over sequence length y_t^(m) = atten_heads(x_t^(m))\n",
    "        # x = x + self.dropout(self.feed_forward(self.ln2(x))) # communication across embedding space x_t^(m+1) = mlp(y_t^(m))\n",
    "        return x\n",
    "class TransformerLM_2(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6, n_layers=6):\n",
    "        \"\"\"\n",
    "          Args:\n",
    "              vocab_size: int, number of tokens in the vocabulary (V)\n",
    "              context_window_size: int, size of the context window (T)\n",
    "              embed_size: int, embedding size (D)\n",
    "              num_heads: int, number of heads (H)\n",
    "              n_layers: int, number of layers (M)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding_table = nn.Embedding(context_window_size, embed_size)\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock_2(vocab_size,\n",
    "                             context_window_size,\n",
    "                             embed_size=embed_size,\n",
    "                             num_heads=num_heads)\n",
    "            for _ in range(n_layers)]) # chains output of one block to input of next. Output of self.blocks is x_t^M\n",
    "\n",
    "        # final layer norm\n",
    "        self.ln_f = nn.LayerNorm(embed_size)\n",
    "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # good initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, token_ids, targets=None):\n",
    "        \"\"\"\n",
    "        Agrgs:\n",
    "            token_ids: tensor of integers, provides the contet, shape (B, T)\n",
    "            targets: tensor of integers, provides the tokens we are preidcitng, shape (B, T)\n",
    "        \"\"\"\n",
    "        B, T = token_ids.shape\n",
    "\n",
    "        # token_ids and targets are both (B, T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(token_ids) # (B, T, D)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, D)\n",
    "        x = tok_emb + pos_emb # (B, T, D)\n",
    "\n",
    "        # TODO: your code below\n",
    "        x = self.blocks(x) # instead of just x=self.attention_head(x), x is processed through a sequence of transformer blocks, each first processed by \"self attn\" then by mlp.\n",
    "        # this x is x_t^M in Scott's notes\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fcn(logits.reshape(-1, self.vocab_size), targets.reshape(-1)) # as before\n",
    "\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, token_ids, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_ids: tensor of integers forming the context, shape (B, T)\n",
    "            max_new_tokens: int, max number of tokens to generate\n",
    "\n",
    "        \"\"\"\n",
    "        # if token_ids.shape[0] ==1:\n",
    "        #     token_ids = token_ids.unsqueeze(0)\n",
    "        #     token_ids = token_ids.unsqueeze(0)\n",
    "\n",
    "        for i in range(max_new_tokens):\n",
    "            B, T = token_ids.shape # (batch size, length)\n",
    "            t = min(CONTEXT_WINDOW_SIZE,T)\n",
    "            tok_emb = self.token_embedding_table(token_ids[:,-t:]).reshape(B,t,-1) # (B,T,D)\n",
    "            pos_emb = self.position_embedding_table(torch.arange(t, device=device)) # (T,D)\n",
    "            x = tok_emb + pos_emb # (B,T,D)\n",
    "            x = self.blocks(x) # (B,T,D)\n",
    "            x= self.ln_f(x)\n",
    "            logits = self.lm_head(x) # (B,T,V)\n",
    "            # if len(logits.shape) == 2:\n",
    "            #     new_token = torch.argmax(logits[-1,:])\n",
    "            # else:\n",
    "            #     new_token = torch.argmax(logits[:,-1],dim=1)\n",
    "            # new_token = torch.argmax(logits[:,-1],dim=1) # best token for each batch\n",
    "            new_token = torch.distributions.Categorical(logits=logits[:,-1]).sample()\n",
    "            # token_ids = torch.cat([token_ids,new_token.unsqueeze(0)],dim=1)\n",
    "            token_ids = torch.cat([token_ids,new_token.unsqueeze(1)],dim=1)\n",
    "\n",
    "    # return decode(token_ids.tolist())\n",
    "        return token_ids\n",
    "        # pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, context_window_size, num_heads, head_size, embed_size=384):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            context_window_size: int, number of tokens considered in the past for attention (T)\n",
    "            num_heads: int, number of heads (H)\n",
    "            head_size: int, size of the head embedding dimension\n",
    "            embed_size: int, size of the token embedding dimension\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO, your code below\n",
    "        self.heads = nn.ModuleList([Head(head_size, context_window_size, embed_size) for _ in range(num_heads)])\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # could also learn a map from the concatenated heads to the output space ?\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x): # x is (B,T,D)\n",
    "        # TODO, your code below\n",
    "        heads = [head(x) for head in self.heads] # list of (B,T,D)\n",
    "        # approach 1\n",
    "        avg_embeddings = torch.stack(heads).sum(dim=0) # (B,T,D) - just sum across heads elementwise for each token\n",
    "        return avg_embeddings\n",
    "        # pass\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity\n",
    "        Given to you, you don't need to write any code here!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4 * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\" Transformer block: communication across sequence length, followed by communication across embedding space\n",
    "        Uses multi-headed attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        # TODO: your code below\n",
    "        self.feed_forward = FeedForward(embed_size) # acts along rows x_t^(m+1) = mlp(y_t^(m))\n",
    "        self.atten_heads = MultiHeadAttention(context_window_size, num_heads, embed_size//num_heads, embed_size) # as before\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.atten_heads(self.ln1(x)) # communication over sequence length y_t^(m) = atten_heads(x_t^(m))\n",
    "\n",
    "        x = x + self.feed_forward(self.ln2(x)) # communication across embedding space x_t^(m+1) = mlp(y_t^(m))\n",
    "        #print(f\"Before ln1: {x.shape}, {x.dtype}\")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model for var attn\n",
    "class MaskedHead(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, context_window_size, embed_size=384):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          head_size: int, size of the head embedding dimension (K)\n",
    "          context_window_size: int, number of tokens considered in the past for attention (T)\n",
    "          embed_size: int, size of the token embedding dimension (D)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.z = nn.Parameter(torch.randn(1)) # learnable parameter controlling attention span\n",
    "        # come back to this, not sure what the best way to initialize this is\n",
    "        self.R = 10\n",
    "        self.S = 100\n",
    "        self.lambda_reg = 0#1e-20\n",
    "        self.cached_mask = None\n",
    "        self.cached_T = None\n",
    "        # self.cached_z = None\n",
    "\n",
    "        # not a param of the model, so registered as a buffer\n",
    "        self.register_buffer('tril', torch.tril(\n",
    "            torch.ones(context_window_size, context_window_size))) # lower diagonal matrix\n",
    "\n",
    "\n",
    "\n",
    "    def compute_masking_matrix(self, z, R, T):\n",
    "        if self.cached_mask is not None and self.cached_T == T and torch.allclose(self.cached_z, z):\n",
    "            return self.cached_mask\n",
    "\n",
    "        positions = torch.arange(T, device=z.device)\n",
    "        distance_matrix = positions.unsqueeze(1) - positions.unsqueeze(0)\n",
    "        distance_matrix = distance_matrix.abs()  # Make sure to take the absolute value\n",
    "\n",
    "        # Correctly implement m_z(x) = min[max[1/R(R + z - x), 0], 1]\n",
    "        mask = (1 / R) * (R + z - distance_matrix)\n",
    "        mask = torch.clamp(mask, min=0, max=1)\n",
    "\n",
    "        self.cached_mask = mask\n",
    "        self.cached_T = T\n",
    "        self.cached_z = z.detach().clone()\n",
    "\n",
    "        return mask  # Shape: (T, T)\n",
    "\n",
    "    def masked_attention(self, attn_scores):\n",
    "        B, T, _ = attn_scores.shape\n",
    "        mask = self.compute_masking_matrix(self.z, self.R, T).unsqueeze(0)\n",
    "\n",
    "        # For values where mask is 0, we want to ensure they get no attention\n",
    "        # Setting the log probability to -inf before softmax achieves this\n",
    "        log_mask = torch.log(mask.clamp(min=1e-10))\n",
    "        masked_scores = attn_scores + log_mask\n",
    "        attn_probs = torch.softmax(masked_scores, dim=-1)\n",
    "\n",
    "        return attn_probs\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x: (B,T,D) tensor of token embeddings\n",
    "\n",
    "        Returns:\n",
    "          (B,T,D) tensor of attention-weighted token embeddings\n",
    "        \"\"\"\n",
    "\n",
    "        attn_weights = self.query(x) @ self.key(x).transpose(-2,-1) # (B,T,T)\n",
    "\n",
    "        attn_weights = attn_weights.masked_fill((self.tril==0)[0:attn_weights.shape[1],0:attn_weights.shape[2]], float('-inf')) # (B,T,T)\n",
    "        masked_attn_weights = self.masked_attention(attn_weights)\n",
    "        avg_embeddings = torch.einsum('bij,bjd->bid', masked_attn_weights, self.value(x))\n",
    "        l1_penalty = self.lambda_reg * self.z.abs().sum()\n",
    "\n",
    "        return avg_embeddings,l1_penalty\n",
    "\n",
    "\n",
    "class MultiHeadAttention_Masked(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, context_window_size, num_heads, head_size, embed_size=384):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            context_window_size: int, number of tokens considered in the past for attention (T)\n",
    "            num_heads: int, number of heads (H)\n",
    "            head_size: int, size of the head embedding dimension\n",
    "            embed_size: int, size of the token embedding dimension\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO, your code below\n",
    "        # had to change to module list bc l1_penalty now\n",
    "        self.heads = nn.ModuleList([MaskedHead(head_size, context_window_size, embed_size) for _ in range(num_heads)])\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "\n",
    "    def forward(self, x): # x is (B,T,D)\n",
    "        # TODO, your code below\n",
    "        heads = [head(x) for head in self.heads] # list of (B,T,D)\n",
    "        # approach 1\n",
    "        avg_embeddings = torch.stack([h[0] for h in heads]).sum(dim=0) # (B,T,D) - just sum across heads elementwise for each token\n",
    "        total_l1_penalty = sum(h[1] for h in heads) / self.num_heads  # Average L1 across heads\n",
    "\n",
    "        return avg_embeddings, total_l1_penalty\n",
    "        # pass\n",
    "class TransformerBlock_VarAttn(nn.Module):\n",
    "    \"\"\" Transformer block: communication across sequence length, followed by communication across embedding space\n",
    "        Uses multi-headed attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        # TODO: your code below\n",
    "        self.feed_forward = FeedForward(embed_size) # acts along rows x_t^(m+1) = mlp(y_t^(m))\n",
    "        self.atten_heads = MultiHeadAttention_Masked(context_window_size, num_heads, embed_size//num_heads, embed_size) # as before\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        # for dropout\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) # takes a dropout rate arg\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, l1_penalty = self.atten_heads(self.ln1(x))\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = x + self.dropout(self.feed_forward(self.ln2(x)))\n",
    "        return x, l1_penalty\n",
    "\n",
    "class TransformerLM_VarAttn(nn.Module):\n",
    "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6, n_layers=6):\n",
    "        \"\"\"\n",
    "          Args:\n",
    "              vocab_size: int, number of tokens in the vocabulary (V)\n",
    "              context_window_size: int, size of the context window (T)\n",
    "              embed_size: int, embedding size (D)\n",
    "              num_heads: int, number of heads (H)\n",
    "              n_layers: int, number of layers (M)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding_table = nn.Embedding(context_window_size, embed_size)\n",
    "        # module list again bc l1_penalty\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock_VarAttn(vocab_size, context_window_size, embed_size=embed_size, num_heads=num_heads)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(embed_size)\n",
    "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Good initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, token_ids, targets=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_ids: (B, T) tensor of integers, provides the context\n",
    "            targets: (B, T) tensor of integers, provides the tokens we are predicting\n",
    "        \"\"\"\n",
    "        B, T = token_ids.shape\n",
    "        tok_emb = self.token_embedding_table(token_ids)  # (B, T, D)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=token_ids.device))  # (T, D)\n",
    "        x = tok_emb + pos_emb  # (B, T, D)\n",
    "\n",
    "        total_l1_penalty = 0\n",
    "        # does the sequential processing manually since using module list instead of sequential\n",
    "        for block in self.blocks:\n",
    "            x, l1_penalty = block(x)\n",
    "            total_l1_penalty += l1_penalty\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fcn(logits.reshape(-1, self.vocab_size), targets.reshape(-1)) + total_l1_penalty\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, token_ids, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_ids: (B, T) tensor of integers forming the context.\n",
    "            max_new_tokens: int, max number of tokens to generate.\n",
    "        \"\"\"\n",
    "        for i in range(max_new_tokens):\n",
    "            B, T = token_ids.shape  # (B, T)\n",
    "            t = min(T, self.position_embedding_table.num_embeddings)\n",
    "            tok_emb = self.token_embedding_table(token_ids[:, -t:]).reshape(B, t, -1)  # (B, T, D)\n",
    "            pos_emb = self.position_embedding_table(torch.arange(t, device=token_ids.device))  # (T, D)\n",
    "            x = tok_emb + pos_emb  # (B, T, D)\n",
    "            for block in self.blocks:\n",
    "                x, _ = block(x)\n",
    "            x = self.ln_f(x)\n",
    "            logits = self.lm_head(x)  # (B, T, V)\n",
    "            new_token = torch.distributions.Categorical(logits=logits[:, -1]).sample()\n",
    "            token_ids = torch.cat([token_ids, new_token.unsqueeze(1)], dim=1)\n",
    "        return token_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6, n_layers=6):\n",
    "        \"\"\"\n",
    "          Args:\n",
    "              vocab_size: int, number of tokens in the vocabulary (V)\n",
    "              context_window_size: int, size of the context window (T)\n",
    "              embed_size: int, embedding size (D)\n",
    "              num_heads: int, number of heads (H)\n",
    "              n_layers: int, number of layers (M)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding_table = nn.Embedding(context_window_size, embed_size)\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(vocab_size,\n",
    "                             context_window_size,\n",
    "                             embed_size=embed_size,\n",
    "                             num_heads=num_heads)\n",
    "            for _ in range(n_layers)]) # chains output of one block to input of next. Output of self.blocks is x_t^M\n",
    "\n",
    "        # final layer norm\n",
    "        self.ln_f = nn.LayerNorm(embed_size)\n",
    "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # good initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, token_ids, targets=None):\n",
    "        \"\"\"\n",
    "        Agrgs:\n",
    "            token_ids: tensor of integers, provides the contet, shape (B, T)\n",
    "            targets: tensor of integers, provides the tokens we are preidcitng, shape (B, T)\n",
    "        \"\"\"\n",
    "        B, T = token_ids.shape\n",
    "\n",
    "        # token_ids and targets are both (B, T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(token_ids) # (B, T, D)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, D)\n",
    "        x = tok_emb + pos_emb # (B, T, D)\n",
    "\n",
    "        # TODO: your code below\n",
    "        x = self.blocks(x) # instead of just x=self.attention_head(x), x is processed through a sequence of transformer blocks, each first processed by \"self attn\" then by mlp.\n",
    "        # this x is x_t^M in Scott's notes\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fcn(logits.reshape(-1, self.vocab_size), targets.reshape(-1)) # as before\n",
    "\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, token_ids, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_ids: tensor of integers forming the context, shape (B, T)\n",
    "            max_new_tokens: int, max number of tokens to generate\n",
    "\n",
    "        \"\"\"\n",
    "        # if token_ids.shape[0] ==1:\n",
    "        #     token_ids = token_ids.unsqueeze(0)\n",
    "        #     token_ids = token_ids.unsqueeze(0)\n",
    "        #print(token_ids)\n",
    "        for i in range(max_new_tokens):\n",
    "            B, T = token_ids.shape # (batch size, length)\n",
    "            t = min(CONTEXT_WINDOW_SIZE,T)\n",
    "            #print(t)\n",
    "            tok_emb = self.token_embedding_table(token_ids[:,-t:]).reshape(B,t,-1) # (B,T,D)\n",
    "            pos_emb = self.position_embedding_table(torch.arange(t, device=device)) # (T,D)\n",
    "            x = tok_emb + pos_emb # (B,T,D)\n",
    "            #print(x.shape)\n",
    "\n",
    "            x = self.blocks(x) # (B,T,D)\n",
    "            x= self.ln_f(x)\n",
    "\n",
    "            logits = self.lm_head(x) # (B,T,V)\n",
    "            # if len(logits.shape) == 2:\n",
    "            #     new_token = torch.argmax(logits[-1,:])\n",
    "            # else:\n",
    "            #     new_token = torch.argmax(logits[:,-1],dim=1)\n",
    "            # new_token = torch.argmax(logits[:,-1],dim=1) # best token for each batch\n",
    "            new_token = torch.distributions.Categorical(logits=logits[:,-1]).sample()\n",
    "            # token_ids = torch.cat([token_ids,new_token.unsqueeze(0)],dim=1)\n",
    "            token_ids = torch.cat([token_ids,new_token.unsqueeze(1)],dim=1)\n",
    "\n",
    "    # return decode(token_ids.tolist())\n",
    "        return token_ids\n",
    "        # pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/52/w3mmgby125s0y29t28_yzw540000gn/T/ipykernel_2604/2768099232.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"/Users/ldavis2000/Documents/Winter Quarter/305B/Baseline_model.pth\",\n"
     ]
    }
   ],
   "source": [
    "#if model is variable attention\n",
    "checkpoint = torch.load(\"/Users/ldavis2000/Documents/Winter Quarter/305B/Baseline_model.pth\",\n",
    "                       map_location=torch.device('cpu'))\n",
    "#update model name for loading\n",
    "\n",
    "trans = checkpoint[\"model\"]\n",
    "loss_list = checkpoint[\"loss_list\"]\n",
    "# Set the model to evaluation mode \n",
    "# trans.eval()\n",
    "\n",
    "# If using a GPU, move the model to the device (e.g., CUDA)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trans.to(device)\n",
    "\n",
    "tlm = trans.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remaking the training and test splits for security\n",
    "n = len(data)\n",
    "train_chars = data[:int(n*0.9)]\n",
    "val_chars = data[int(n*0.9):]\n",
    "\n",
    "# encode both to integers\n",
    "train_data = encodeBiandTri(train_chars)\n",
    "val_data = encodeBiandTri(val_chars)\n",
    "\n",
    "# cast as torch tensors\n",
    "train_data = torch.tensor(train_data)\n",
    "val_data = torch.tensor(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logits function calculator\n",
    "def logits(context: str, predicted: str):\n",
    "\n",
    "    #Encoding inputs\n",
    "    context_tokens = tokenizer.encode(context, add_special_tokens=False)\n",
    "    pred_tokens = tokenizer.encode(predicted, add_special_tokens=False)\n",
    "\n",
    "    # Combine context and predicted tokens\n",
    "    input_tokens = context_tokens + pred_tokens\n",
    "    input_tensor = torch.tensor([input_tokens])\n",
    "    input_tensor = input_tensor.to(\"cpu\")\n",
    "    \n",
    "    \n",
    "    #logits output no autograd for mem\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        logits = outputs.logits  \n",
    "\n",
    "    #logits for the predicted tokens\n",
    "    \n",
    "    pred_logits = logits[0, -len(pred_tokens)-1:-1, :]\n",
    "\n",
    "    return pred_logits, pred_tokens  \n",
    "\n",
    "context = \"The quick brown fox\"\n",
    "predicted = \"jump overs the lazy dogg\"\n",
    "truth = \"jumps over the lazy dog\"\n",
    "\n",
    "logits_out_truth, logits_out_tokens = logits(context, truth)\n",
    "\n",
    "#logits_out_pred = logits(context,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 25 trials at 0\n",
      "Finished 25 trials at 5000\n",
      "Finished 25 trials at 10000\n",
      "Finished 25 trials at 15000\n",
      "Finished 25 trials at 20000\n",
      "Finished 25 trials at 25000\n",
      "Finished 25 trials at 30000\n",
      "Finished 25 trials at 35000\n",
      "Finished 25 trials at 40000\n",
      "Finished 25 trials at 45000\n",
      "Finished 25 trials at 50000\n",
      "Finished 25 trials at 55000\n",
      "Finished 25 trials at 60000\n",
      "Finished 25 trials at 65000\n",
      "Finished 25 trials at 70000\n",
      "Finished 25 trials at 75000\n",
      "Finished 25 trials at 80000\n",
      "Finished 25 trials at 85000\n",
      "Finished 25 trials at 90000\n",
      "Finished 25 trials at 95000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "#context_chars-decode2(pred_output)\n",
    "def pred_par_only(s1, s2):\n",
    "\n",
    "    for i in range(len(s1)):\n",
    "        if s2.startswith(s1[i:]):  \n",
    "            return s2[len(s1[i:]):]  \n",
    "    return s2  \n",
    "\n",
    "N_rep = 10 #number of repeated simulations\n",
    "\n",
    "con_length = 100 #context length of 100\n",
    "\n",
    "pred_length = 20 #length of predicted output\n",
    "\n",
    "#context is given in integers\n",
    "\n",
    "#using 20 different contexts\n",
    "\n",
    "con_start = 5000*torch.arange(20)\n",
    "\n",
    "mean = []\n",
    "\n",
    "for i1 in con_start:\n",
    "    \n",
    "    context_chars = val_chars[i1:(i1+con_length)] #is a string\n",
    "    \n",
    "    context = torch.tensor(encodeBiandTri(context_chars)).reshape(1,-1)\n",
    "    #is tokens in tensor\n",
    "    \n",
    "    mean_start = []\n",
    "    \n",
    "    for j1 in range(N_rep):\n",
    "        \n",
    "        pred_output = (tlm.generate(context, max_new_tokens=pred_length)[0].tolist())\n",
    "        #test is just context and outputs 50 tokens\n",
    "        \n",
    "        predicted_extra = pred_par_only(context_chars, decode2(pred_output))\n",
    "        #only the generated text\n",
    "        \n",
    "        pred_text = predicted_extra[:pred_length]\n",
    "        #generated text of 50 characters\n",
    "        \n",
    "        encode_pred_text = encodeBiandTri(pred_text)\n",
    "        #encoding above to integers\n",
    "        \n",
    "        #new_element = (torch.tensor([[encode_pred_text]])\n",
    "        #turning to torch tensor\n",
    "        \n",
    "        logits_inner = []\n",
    "        \n",
    "        for i in range(len(encode_pred_text)-1):\n",
    "            \n",
    "            pred_context = decode2(encode_pred_text[0:(i+1)])\n",
    "            #previous context to predict on\n",
    "                                   \n",
    "            context_chars_tmp = context_chars + pred_context\n",
    "            \n",
    "            #to predict is next character\n",
    "            \n",
    "            pred_logits, pred_tokens = logits(context_chars_tmp,\\\n",
    "                                              decode2(encode_pred_text[(i+1):(i+2)]))\n",
    "            #predicted xtra only to first 50 characters\n",
    "            \n",
    "            logits_inner.append((pred_logits[:,pred_tokens].squeeze(1))[0])\n",
    "\n",
    "            #tmp = torch.tensor([pred_logits[idx,token].item() for idx, token in enumerate(pred_tokens)])\n",
    "        \n",
    "        first_elements = [t[0].item() if t.numel() > 1 else t.item() for t in logits_inner]\n",
    "        \n",
    "        mean_start.append(torch.tensor(first_elements).mean()) \n",
    "        #means of tokens we use\n",
    "        \n",
    "    mean.append(torch.tensor(mean_start).mean())\n",
    "        \n",
    "    print(f\"Finished 25 trials at {i1}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##UPDATE FILE NAME TO SAVE CORRECTLY after compuatation\n",
    "mean_base = torch.tensor(mean).clone()\n",
    "\n",
    "#from google.colab import files\n",
    "# Save tensor\n",
    "torch.save(mean_base, \"/Users/ldavis2000/Documents/Winter Quarter/305B/base_means_new.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/52/w3mmgby125s0y29t28_yzw540000gn/T/ipykernel_3633/2625887310.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  var_means_0_attn =torch.load(\"/Users/ldavis2000/Documents/Winter Quarter/305B/0_var_means_new.pth\")\n",
      "/var/folders/52/w3mmgby125s0y29t28_yzw540000gn/T/ipykernel_3633/2625887310.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mean_var_attn_50 =torch.load(\"/Users/ldavis2000/Documents/Winter Quarter/305B/50_var_means_new.pth\")\n",
      "/var/folders/52/w3mmgby125s0y29t28_yzw540000gn/T/ipykernel_3633/2625887310.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mean_var_attn_100 =torch.load(\"/Users/ldavis2000/Documents/Winter Quarter/305B/100_var_means_new.pth\")\n",
      "/var/folders/52/w3mmgby125s0y29t28_yzw540000gn/T/ipykernel_3633/2625887310.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mean_var_attn_250 =torch.load(\"/Users/ldavis2000/Documents/Winter Quarter/305B/250_var_means_new.pth\")\n",
      "/var/folders/52/w3mmgby125s0y29t28_yzw540000gn/T/ipykernel_3633/2625887310.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mean_reg_attn_50 = torch.load(\"/Users/ldavis2000/Documents/Winter Quarter/305B/50_Attn_means_new.pth\")\n",
      "/var/folders/52/w3mmgby125s0y29t28_yzw540000gn/T/ipykernel_3633/2625887310.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mean_reg_attn_100 = torch.load(\"/Users/ldavis2000/Documents/Winter Quarter/305B/100_Attn_means_new.pth\")\n",
      "/var/folders/52/w3mmgby125s0y29t28_yzw540000gn/T/ipykernel_3633/2625887310.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mean_reg_attn_250 = torch.load(\"/Users/ldavis2000/Documents/Winter Quarter/305B/250_Attn_means_new.pth\")\n",
      "/var/folders/52/w3mmgby125s0y29t28_yzw540000gn/T/ipykernel_3633/2625887310.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mean_base_load = torch.load(\"/Users/ldavis2000/Documents/Winter Quarter/305B/base_means_new.pth\")\n"
     ]
    }
   ],
   "source": [
    "#load data \n",
    "var_means_0_attn =torch.load(\"/Users/ldavis2000/Documents/Winter Quarter/305B/0_var_means_new.pth\")\n",
    "\n",
    "mean_var_attn_50 =torch.load(\"/Users/ldavis2000/Documents/Winter Quarter/305B/50_var_means_new.pth\")\n",
    "\n",
    "mean_var_attn_100 =torch.load(\"/Users/ldavis2000/Documents/Winter Quarter/305B/100_var_means_new.pth\")\n",
    "\n",
    "mean_var_attn_250 =torch.load(\"/Users/ldavis2000/Documents/Winter Quarter/305B/250_var_means_new.pth\")\n",
    "\n",
    "mean_reg_attn_50 = torch.load(\"/Users/ldavis2000/Documents/Winter Quarter/305B/50_Attn_means_new.pth\")\n",
    "\n",
    "mean_reg_attn_100 = torch.load(\"/Users/ldavis2000/Documents/Winter Quarter/305B/100_Attn_means_new.pth\")\n",
    "\n",
    "mean_reg_attn_250 = torch.load(\"/Users/ldavis2000/Documents/Winter Quarter/305B/250_Attn_means_new.pth\")\n",
    "\n",
    "mean_base_load = torch.load(\"/Users/ldavis2000/Documents/Winter Quarter/305B/base_means_new.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#to update\n",
    "pred_tensors_total = [mean_base_load,mean_reg_attn_50,mean_reg_attn_100,mean_reg_attn_250,\n",
    "                      var_means_0_attn,mean_var_attn_50,mean_var_attn_100,mean_var_attn_250]\n",
    "\n",
    "data = [t.tolist() for t in pred_tensors_total]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 3))\n",
    "box = plt.boxplot(data, patch_artist=False)\n",
    "\n",
    "\n",
    "legend_labels = ['Base', 'Reg Attn 50', 'Reg Attn 100',\n",
    "                 'Reg Attn 250','Var Attn', 'Var Attn 50', 'Var Attn 100', 'Var Attn 250']\n",
    "ax.set_xticks(range(1, 8+1))\n",
    "ax.set_xticklabels(legend_labels)\n",
    "ax.grid()\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Logits\")\n",
    "plt.title(\"Mean GPT2-XL Logits on Predicted Data\", fontweight=\"bold\")\n",
    "\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.8, 1]) \n",
    "\n",
    "\n",
    "plt.savefig(\"GPTLogits.png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msm\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformula\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msmf\u001b[39;00m\n\u001b[1;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModelSize\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m50\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m50\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m250\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m250\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVarAttention\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogits\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmean_base_load\u001b[49m\u001b[43m)\u001b[49m,\u001b[38;5;28mfloat\u001b[39m(var_means_0_attn),\u001b[38;5;28mfloat\u001b[39m(mean_reg_attn_50),\u001b[38;5;28mfloat\u001b[39m(mean_var_attn_50),\n\u001b[1;32m      9\u001b[0m               \u001b[38;5;28mfloat\u001b[39m(mean_reg_attn_100),\u001b[38;5;28mfloat\u001b[39m(mean_var_attn_100),\u001b[38;5;28mfloat\u001b[39m(mean_reg_attn_250),\u001b[38;5;28mfloat\u001b[39m(mean_var_attn_250)]\n\u001b[1;32m     10\u001b[0m }\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Create DataFrame\u001b[39;00m\n\u001b[1;32m     13\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "data = {\n",
    "    \"ModelSize\": [\"base\", \"base\", \"50\", \"50\", \"100\", \"100\", \"250\", \"250\"],\n",
    "    \"VarAttention\": [0, 1, 0, 1, 0, 1, 0,1],\n",
    "    \"Logits\": [float(mean_base_load),float(var_means_0_attn),float(mean_reg_attn_50),float(mean_var_attn_50),\n",
    "              float(mean_reg_attn_100),float(mean_var_attn_100),float(mean_reg_attn_250),float(mean_var_attn_250)]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert categorical variables\n",
    "df[\"ModelSize\"] = df[\"ModelSize\"].astype(\"category\")\n",
    "df[\"VarAttention\"] = df[\"VarAttention\"].astype(\"category\")\n",
    "\n",
    "# Fit Two-Way ANOVA model\n",
    "model = smf.ols(\"Logits ~ C(ModelSize) + C(VarAttention) + C(ModelSize):C(VarAttention)\", data=df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Print ANOVA table\n",
    "print(anova_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0856, 2.2239, 2.5666, 2.1195, 2.4029, 1.6839, 2.6417, 2.1999, 2.7874,\n",
       "        2.0976, 2.1047, 1.6660, 3.4938, 1.9579, 1.7235, 2.8108, 2.1577, 2.6914,\n",
       "        2.3444, 2.5079])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_base_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 Logits   R-squared:                       0.115\n",
      "Model:                            OLS   Adj. R-squared:                  0.072\n",
      "Method:                 Least Squares   F-statistic:                     2.647\n",
      "Date:                Fri, 14 Mar 2025   Prob (F-statistic):             0.0133\n",
      "Time:                        11:18:15   Log-Likelihood:                -126.89\n",
      "No. Observations:                 150   AIC:                             269.8\n",
      "Df Residuals:                     142   BIC:                             293.9\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=============================================================================================================\n",
      "                                                coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "Intercept                                     2.4004      0.130     18.525      0.000       2.144       2.657\n",
      "C(ModelSize)[T.250]                           0.0516      0.183      0.282      0.779      -0.311       0.414\n",
      "C(ModelSize)[T.50]                            0.3103      0.183      1.693      0.093      -0.052       0.673\n",
      "C(ModelSize)[T.base]                         -0.0871      0.183     -0.475      0.635      -0.449       0.275\n",
      "C(VarAttention)[T.1]                          0.1378      0.183      0.752      0.453      -0.224       0.500\n",
      "C(ModelSize)[T.250]:C(VarAttention)[T.1]     -0.3567      0.259     -1.377      0.171      -0.869       0.156\n",
      "C(ModelSize)[T.50]:C(VarAttention)[T.1]       0.0261      0.259      0.101      0.920      -0.486       0.538\n",
      "C(ModelSize)[T.base]:C(VarAttention)[T.1]     0.1727      0.290      0.596      0.552      -0.400       0.745\n",
      "==============================================================================\n",
      "Omnibus:                       11.899   Durbin-Watson:                   2.815\n",
      "Prob(Omnibus):                  0.003   Jarque-Bera (JB):               12.723\n",
      "Skew:                           0.601   Prob(JB):                      0.00173\n",
      "Kurtosis:                       3.769   Cond. No.                         12.1\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                                 sum_sq     df         F    PR(>F)\n",
      "C(ModelSize)                   4.567571    3.0  4.533715  0.004564\n",
      "C(VarAttention)                0.228876    1.0  0.681540  0.410442\n",
      "C(ModelSize):C(VarAttention)   1.351537    3.0  1.341520  0.263362\n",
      "Residual                      47.686795  142.0       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "logit_tensors = {\n",
    "    \"base_0\": mean_base_load.tolist(),\n",
    "    \"base_1\": var_means_0_attn.tolist(),\n",
    "    \"50_0\": mean_reg_attn_50.tolist(),\n",
    "    \"50_1\": mean_var_attn_50.tolist(),\n",
    "    \"100_0\": mean_reg_attn_100.tolist(),\n",
    "    \"100_1\": mean_var_attn_100.tolist(),\n",
    "    \"250_0\": mean_reg_attn_250.tolist(),\n",
    "    \"250_1\": mean_var_attn_250.tolist()\n",
    "}\n",
    "\n",
    "data = []\n",
    "for key, logits in logit_tensors.items():\n",
    "    model_size, var_attention = key.split(\"_\") \n",
    "    for logit in logits:\n",
    "        data.append({\"ModelSize\": model_size, \"VarAttention\": int(var_attention), \"Logits\": logit})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "df[\"ModelSize\"] = df[\"ModelSize\"].astype(\"category\")\n",
    "df[\"VarAttention\"] = df[\"VarAttention\"].astype(\"category\")\n",
    "\n",
    "model = smf.ols(\"Logits ~ C(ModelSize) + C(VarAttention) + C(ModelSize):C(VarAttention)\", data=df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "print(model.summary())\n",
    "print(anova_table)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10 (STATS 305B)",
   "language": "python",
   "name": "305b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
