{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Note: The proposed improvements to the basic transformer model are below the baseline version of the model in this file. (See Part 2)"
      ],
      "metadata": {
        "id": "be-pPr5VeA8O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "K932VmrEc4lh"
      },
      "outputs": [],
      "source": [
        "# torch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "a\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import requests\n",
        "import os\n",
        "\n",
        "torch.manual_seed(305)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "A_Z5Jh74DH_E"
      },
      "outputs": [],
      "source": [
        "# Global hyperparameters\n",
        "SMALL_ITERS = 1000\n",
        "LARGE_ITERS = 2000\n",
        "EVAL_ITERS = 100\n",
        "CONTEXT_WINDOW_SIZE = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53dGz7ExDkUv",
        "outputId": "dd6cbf18-49bf-4e72-c251-82597da0463d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n"
          ]
        }
      ],
      "source": [
        "# download the tiny shakespeare dataset\n",
        "input_file_path = 'input.txt'\n",
        "\n",
        "if not os.path.exists(input_file_path):\n",
        "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "    with open(input_file_path, 'w') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r') as f:\n",
        "    data = f.read()\n",
        "print(f\"length of dataset in characters: {len(data):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2HuC2F9p_4K",
        "outputId": "21509c72-3064-4c96-d955-af76a83d5c75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n"
          ]
        }
      ],
      "source": [
        "# get all the unique characters that occur in this text\n",
        "chars = sorted(list(set(data)))\n",
        "vocab_size = len(chars)\n",
        "print(\"all the unique characters:\", ''.join(chars))\n",
        "print(f\"vocab size: {vocab_size:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEWx5jnKqDzT",
        "outputId": "eafecc9e-e902-47bc-8338-64dd3c7a8d8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n"
          ]
        }
      ],
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "def encode(s):\n",
        "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "def decode(l):\n",
        "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# create the train and test splits\n",
        "n = len(data)\n",
        "train_chars = data[:int(n*0.9)]\n",
        "val_chars = data[int(n*0.9):]\n",
        "\n",
        "# encode both to integers\n",
        "train_data = encode(train_chars)\n",
        "val_data = encode(val_chars)\n",
        "\n",
        "# cast as torch tensors\n",
        "train_data = torch.tensor(train_data)\n",
        "val_data = torch.tensor(val_data)\n",
        "\n",
        "print(f\"train has {len(train_data):,} tokens\")\n",
        "print(f\"val has {len(val_data):,} tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "AkAD0PfiEfjG"
      },
      "outputs": [],
      "source": [
        "# function for getting batches of data\n",
        "def get_batch(split, context_window_size, device, batch_size=32):\n",
        "    \"\"\"\n",
        "    generate a small batch of data of inputs x and targets y\n",
        "\n",
        "    Args:\n",
        "        split: 'train' or 'val'\n",
        "        device: 'cpu' or 'cuda' (should be 'cuda' if available)\n",
        "    \"\"\"\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - context_window_size, (batch_size,))  # generate a bunch of starting positions\n",
        "    x = torch.stack([data[i:i+context_window_size] for i in ix]) # stack the data from each starting point\n",
        "    y = torch.stack([data[i+1:i+context_window_size+1] for i in ix])\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# helper function for tracking loss during training\n",
        "# given to you\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model, eval_iters, context_window_size, device):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      model: model being evaluated\n",
        "      eval_iters: number of batches to average over\n",
        "      context_window_size: size of the context window\n",
        "      device: 'cpu' or 'cuda' (should be 'cuda' if available)\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split, context_window_size, device)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "O_eBPiT-Yy0q"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size, context_window_size, embed_size=384):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          head_size: int, size of the head embedding dimension (K)\n",
        "          context_window_size: int, number of tokens considered in the past for attention (T)\n",
        "          embed_size: int, size of the token embedding dimension (D)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.head_size = head_size\n",
        "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
        "        self.query = nn.Linear(embed_size, head_size, bias=False)\n",
        "        self.value = nn.Linear(embed_size, embed_size, bias=False)\n",
        "\n",
        "        # not a param of the model, so registered as a buffer\n",
        "        self.register_buffer('tril', torch.tril(\n",
        "            torch.ones(context_window_size, context_window_size))) # lower diagonal matrix\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          x: (B,T,D) tensor of token embeddings\n",
        "\n",
        "        Returns:\n",
        "          (B,T,D) tensor of attention-weighted token embeddings\n",
        "        \"\"\"\n",
        "        # attn_weights = torch.einsum('btd, btd -> btt', self.query(x), self.key(x)) # (B,T)\n",
        "        # attn_weights = self.query(x) @ self.key(x).transpose(1,2) # (B,T,T)\n",
        "\n",
        "        attn_weights = self.query(x) @ self.key(x).transpose(-2,-1) # (B,T,T)\n",
        "        # (self.query(x) @ self.key(x).transpose(1,2)).shape # (B,T,T)\n",
        "        # if len(attn_weights.shape) == 2:\n",
        "        #     attn_weights = attn_weights.unsqueeze(0)\n",
        "        attn_weights = attn_weights.masked_fill((self.tril==0)[0:attn_weights.shape[1],0:attn_weights.shape[2]], float('-inf')) # (B,T,T)\n",
        "        attn_weights = F.softmax(attn_weights/(self.head_size**(1/2)), dim=-1) # (B,T,T)\n",
        "        # if len(attn_weights.shape) == 2:\n",
        "        #     avg_embeddings = torch.einsum('tt, td -> td', attn_weights, self.value(x)) #\n",
        "        # else:\n",
        "\n",
        "        # avg_embeddings = torch.einsum('btt, btd -> btd', attn_weights, self.value(x)) # (B,T,D) # self.value(x) gives (B,T,D), attn_weights gives (B,T)\n",
        "        avg_embeddings = torch.einsum('bij,bjd->bid', attn_weights, self.value(x))\n",
        "        return avg_embeddings\n",
        "\n",
        "\n",
        "\n",
        "        # pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "REr3aWnS1xJL"
      },
      "outputs": [],
      "source": [
        "class SingleHeadedAttentionLM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, context_window_size, head_size, embed_size=384):\n",
        "      \"\"\"\n",
        "      Args:\n",
        "        vocab_size: int, size of the vocabulary (V)\n",
        "        context_window_size: int, number of tokens considered in the past for attention (T)\n",
        "        head_size: int, size of the head embedding dimension (K)\n",
        "        embed_size: int, size of the token embedding dimension (D)\n",
        "      \"\"\"\n",
        "      super().__init__()\n",
        "      self.token_embedding_table = nn.Embedding(vocab_size, embed_size) # X which we will pass to the head\n",
        "      self.position_embedding_table = nn.Embedding(context_window_size, embed_size)\n",
        "      self.context_window_size = context_window_size\n",
        "\n",
        "      # TODO: your code below\n",
        "      self.atten_head = Head(head_size, context_window_size, embed_size)\n",
        "      self.lm_head = nn.Linear(embed_size, vocab_size) # as in part 1.2, we learn a beta matrix to get logits. See \"autoregressive modeling\" slide, W\\in \\R^VxD, st l_{t+1} = Cat(Wx_t)\n",
        "      self.vocab_size = vocab_size\n",
        "      self.embed_size = embed_size\n",
        "\n",
        "    def forward(self, token_ids, targets=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          token_ids: (B, T) token ids that make up the context (batch has size B, each entry\n",
        "                     in the batch has length T)\n",
        "          targets: (B, T) token ids corresponding to the target of each context in token_ids\n",
        "\n",
        "        Returns:\n",
        "          logits: (B, T, V) logits[b,t] gives the length V vector of logits for the next token\n",
        "                   prediction in string b up to t tokens\n",
        "          loss: scalar, negative log likelihood of target given context\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        B, T = token_ids.shape # (batch size, length)\n",
        "        tok_emb = self.token_embedding_table(token_ids) # (B,T,D)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,D)\n",
        "        x = tok_emb + pos_emb # (B,T,D)\n",
        "        x = self.atten_head(x) # (B,T,D)\n",
        "        logits = self.lm_head(x) # (B,T,V)\n",
        "\n",
        "        # TODO: your code here\n",
        "        # logits =\n",
        "        loss_fcn = torch.nn.CrossEntropyLoss()\n",
        "        loss = loss_fcn(logits.reshape(-1, self.vocab_size), targets.reshape(-1)) # as before\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, token_ids, max_new_tokens):\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          token_ids: (B, T) tensor of token ids to provide as context\n",
        "          max_new_tokens: int, maximum number of new tokens to generate\n",
        "\n",
        "        Returns:\n",
        "          (B, T+max_new_tokens) tensor of context with new tokens appended\n",
        "        \"\"\"\n",
        "        #TODO\n",
        "        # your code below\n",
        "        #feels janky to for loop this but dont see a choice?\n",
        "\n",
        "        for i in range(max_new_tokens):\n",
        "            B, T = token_ids.shape # (batch size, length)\n",
        "            tok_emb = self.token_embedding_table(token_ids[:,-self.context_window_size:]) # (B,T,D)\n",
        "            pos_emb = self.position_embedding_table(torch.arange(self.context_window_size, device=device)) # (T,D)\n",
        "            x = tok_emb + pos_emb # (B,T,D)\n",
        "            x = self.atten_head(x) # (B,T,D)\n",
        "            logits = self.lm_head(x) # (B,T,V)\n",
        "            new_token = torch.argmax(logits[:,-1],dim=1) # best token for each batch\n",
        "            token_ids = torch.cat([token_ids,new_token.unsqueeze(1)],dim=1)\n",
        "\n",
        "        # return decode(token_ids.tolist())\n",
        "        return token_ids\n",
        "\n",
        "        # pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "070G_l0E0uG9",
        "outputId": "1dcbf9fe-f21a-46a2-c03c-a3f45d05da16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 11/1000 [00:01<01:16, 12.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.1781, val loss 4.1782\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 200/1000 [00:03<00:10, 77.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|██▏       | 216/1000 [00:04<00:28, 27.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: train loss 2.5814, val loss 2.5836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|███▉      | 396/1000 [00:06<00:07, 76.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████      | 412/1000 [00:08<00:22, 26.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: train loss 2.5230, val loss 2.5427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|█████▉    | 597/1000 [00:10<00:06, 60.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 604/1000 [00:12<00:33, 11.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: train loss 2.4219, val loss 2.4518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|███████▉  | 798/1000 [00:16<00:03, 56.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 81%|████████  | 810/1000 [00:18<00:11, 16.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: train loss 2.3838, val loss 2.4197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 998/1000 [00:20<00:00, 74.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|██████████| 1000/1000 [00:21<00:00, 46.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 999: train loss 2.3572, val loss 2.3973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "embed_size = 384\n",
        "sha_model = SingleHeadedAttentionLM(vocab_size, CONTEXT_WINDOW_SIZE, embed_size, embed_size)\n",
        "sham = sha_model.to(device)\n",
        "learning_rate = 6e-4\n",
        "optimizer = torch.optim.AdamW(sha_model.parameters(), lr=learning_rate)\n",
        "\n",
        "eval_interval = 200\n",
        "\n",
        "loss_list = []\n",
        "\n",
        "for it in tqdm(range(SMALL_ITERS)):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if it % eval_interval == 0 or it == SMALL_ITERS - 1:\n",
        "        print(f\"iteration {it}\")\n",
        "        losses = estimate_loss(sham, EVAL_ITERS, CONTEXT_WINDOW_SIZE, device)\n",
        "        print(\n",
        "            f\"step {it}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\n",
        "        )\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch(\"train\", CONTEXT_WINDOW_SIZE, device)\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = sham(xb, yb)\n",
        "    loss_list.append(loss.detach().item())\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fdq1N6GnOhIQ",
        "outputId": "4940c497-0164-46f4-8724-fac0914481b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'loss and the the the the the the the the the the the the the the the the the the the the the the the the t'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "tokens_out = sha_model.generate(xb, max_new_tokens=100)\n",
        "decode(tokens_out[20,250:].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "yEnrPEvQdYIK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "rB4r2aK1OhIQ",
        "outputId": "026356a7-1e05-45c0-9a08-e33cadaf58ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Loss')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "sns.lineplot(x=range(SMALL_ITERS), y=loss_list)\n",
        "plt.title(\"Loss in training, Single Headed Attention\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u05NFXpo0wWs"
      },
      "source": [
        "---\n",
        "\n",
        "In terms of training loss we definitely see an improvement. In particular the loss decreases to approximately 2, however it is slightly noisier around the optima.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wjXYf-S4Lus"
      },
      "source": [
        "#### 1.3.3: Multi-headed attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIObjMO0Ikp5"
      },
      "source": [
        "##### Question 1.3.3.1: Implement multi-headed attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "6vb8NU_s6Vfg"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, context_window_size, num_heads, head_size, embed_size=384):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            context_window_size: int, number of tokens considered in the past for attention (T)\n",
        "            num_heads: int, number of heads (H)\n",
        "            head_size: int, size of the head embedding dimension\n",
        "            embed_size: int, size of the token embedding dimension\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # TODO, your code below\n",
        "        self.heads = nn.ModuleList([Head(head_size, context_window_size, embed_size) for _ in range(num_heads)])\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # could also learn a map from the concatenated heads to the output space ?\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x): # x is (B,T,D)\n",
        "        # TODO, your code below\n",
        "        heads = [head(x) for head in self.heads] # list of (B,T,D)\n",
        "        # approach 1\n",
        "        avg_embeddings = torch.stack(heads).sum(dim=0) # (B,T,D) - just sum across heads elementwise for each token\n",
        "        return avg_embeddings\n",
        "        # pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFCBRay3IuR3"
      },
      "source": [
        "##### Question 1.3.3.2: Implement a multi-headed attention LM\n",
        "\n",
        "Fill in the code below to create a language model that outputs its logits for next token prediction using multi-headed attention. Train your model for `SMALL_ITERS` training iterations. Compare the results with the single-headed attention model. Do you see an improvement?\n",
        "\n",
        "We get to a train loss of around 2 after 1000 iterations, which takes around 1.5 minutes on a T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "LvWHwcCzI1yr"
      },
      "outputs": [],
      "source": [
        "class MultiHeadedAttentionLM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6):\n",
        "      super().__init__()\n",
        "      self.head_size = embed_size // num_heads\n",
        "      self.context_window_size = context_window_size\n",
        "      # TODO: your code below\n",
        "      self.token_embedding_table = nn.Embedding(vocab_size, embed_size) # X which we will pass to the head\n",
        "      self.position_embedding_table = nn.Embedding(context_window_size, embed_size)\n",
        "      self.atten_head = MultiHeadAttention(context_window_size, num_heads, self.head_size, embed_size)\n",
        "      self.lm_head = nn.Linear(embed_size, vocab_size)\n",
        "      self.vocab_size = vocab_size\n",
        "      self.embed_size = embed_size\n",
        "\n",
        "    def forward(self, token_ids, targets=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          token_ids: (B, T) token ids that make up the context (batch has size B, each entry in the\n",
        "                     batch has length T)\n",
        "          targets: (B, T) token ids corresponding to the target of each context in token_ids\n",
        "\n",
        "        Returns:\n",
        "          logits: (B, T, V), logits[b,t] gives the length V vector of logits for the next token\n",
        "                  prediction in string b up to t tokens\n",
        "          loss: scalar, negative log likelihood of target given context\n",
        "        \"\"\"\n",
        "        # TODO: your code below\n",
        "\n",
        "        B, T = token_ids.shape # (batch size, length)\n",
        "        tok_emb = self.token_embedding_table(token_ids) # (B,T,D)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,D)\n",
        "        x = tok_emb + pos_emb # (B,T,D)\n",
        "        x = self.atten_head(x) # (B,T,D)\n",
        "        logits = self.lm_head(x) # (B,T,V)\n",
        "        loss_fcn = torch.nn.CrossEntropyLoss()\n",
        "        loss = loss_fcn(logits.reshape(-1, self.vocab_size), targets.reshape(-1)) # as before\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, token_ids, max_new_tokens):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          token_ids: (B, T) tensor of token ids to provide as context\n",
        "          max_new_tokens: int, maximum number of new tokens to generate\n",
        "\n",
        "        Returns:\n",
        "          (B, T+max_new_tokens) tensor of context with new tokens appended\n",
        "        \"\"\"\n",
        "        # TODO: your code below\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "1GbGqwKWJzOK"
      },
      "outputs": [],
      "source": [
        "# run this cell to initialize this deep learning module that you should use in the code your write later\n",
        "# you don't need to edit this layer\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity\n",
        "        Given to you, you don't need to write any code here!\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_size):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embed_size, 4 * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * embed_size, embed_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "hUDbIv9eISkf"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\" Transformer block: communication across sequence length, followed by communication across embedding space\n",
        "        Uses multi-headed attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(embed_size)\n",
        "        self.ln2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        # TODO: your code below\n",
        "        self.feed_forward = FeedForward(embed_size) # acts along rows x_t^(m+1) = mlp(y_t^(m))\n",
        "        self.atten_heads = MultiHeadAttention(context_window_size, num_heads, embed_size//num_heads, embed_size) # as before\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.atten_heads(self.ln1(x)) # communication over sequence length y_t^(m) = atten_heads(x_t^(m))\n",
        "        x = x + self.feed_forward(self.ln2(x)) # communication across embedding space x_t^(m+1) = mlp(y_t^(m))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "t2veTg9N3ufJ"
      },
      "outputs": [],
      "source": [
        "class TransformerLM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6, n_layers=6):\n",
        "        \"\"\"\n",
        "          Args:\n",
        "              vocab_size: int, number of tokens in the vocabulary (V)\n",
        "              context_window_size: int, size of the context window (T)\n",
        "              embed_size: int, embedding size (D)\n",
        "              num_heads: int, number of heads (H)\n",
        "              n_layers: int, number of layers (M)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
        "        self.position_embedding_table = nn.Embedding(context_window_size, embed_size)\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            TransformerBlock(vocab_size,\n",
        "                             context_window_size,\n",
        "                             embed_size=embed_size,\n",
        "                             num_heads=num_heads)\n",
        "            for _ in range(n_layers)]) # chains output of one block to input of next. Output of self.blocks is x_t^M\n",
        "\n",
        "        # final layer norm\n",
        "        self.ln_f = nn.LayerNorm(embed_size)\n",
        "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # good initialization\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, token_ids, targets=None):\n",
        "        \"\"\"\n",
        "        Agrgs:\n",
        "            token_ids: tensor of integers, provides the contet, shape (B, T)\n",
        "            targets: tensor of integers, provides the tokens we are preidcitng, shape (B, T)\n",
        "        \"\"\"\n",
        "        B, T = token_ids.shape\n",
        "\n",
        "        # token_ids and targets are both (B, T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(token_ids) # (B, T, D)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, D)\n",
        "        x = tok_emb + pos_emb # (B, T, D)\n",
        "\n",
        "        # TODO: your code below\n",
        "        x = self.blocks(x) # instead of just x=self.attention_head(x), x is processed through a sequence of transformer blocks, each first processed by \"self attn\" then by mlp.\n",
        "        # this x is x_t^M in Scott's notes\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss_fcn = torch.nn.CrossEntropyLoss()\n",
        "        loss = loss_fcn(logits.reshape(-1, self.vocab_size), targets.reshape(-1)) # as before\n",
        "\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, token_ids, max_new_tokens):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            token_ids: tensor of integers forming the context, shape (B, T)\n",
        "            max_new_tokens: int, max number of tokens to generate\n",
        "\n",
        "        \"\"\"\n",
        "        # if token_ids.shape[0] ==1:\n",
        "        #     token_ids = token_ids.unsqueeze(0)\n",
        "        #     token_ids = token_ids.unsqueeze(0)\n",
        "\n",
        "        for i in range(max_new_tokens):\n",
        "            B, T = token_ids.shape # (batch size, length)\n",
        "            t = min(CONTEXT_WINDOW_SIZE,T)\n",
        "            tok_emb = self.token_embedding_table(token_ids[:,-t:]).reshape(B,t,-1) # (B,T,D)\n",
        "            pos_emb = self.position_embedding_table(torch.arange(t, device=device)) # (T,D)\n",
        "            x = tok_emb + pos_emb # (B,T,D)\n",
        "            x = self.blocks(x) # (B,T,D)\n",
        "            x= self.ln_f(x)\n",
        "            logits = self.lm_head(x) # (B,T,V)\n",
        "            # if len(logits.shape) == 2:\n",
        "            #     new_token = torch.argmax(logits[-1,:])\n",
        "            # else:\n",
        "            #     new_token = torch.argmax(logits[:,-1],dim=1)\n",
        "            # new_token = torch.argmax(logits[:,-1],dim=1) # best token for each batch\n",
        "            new_token = torch.distributions.Categorical(logits=logits[:,-1]).sample()\n",
        "            # token_ids = torch.cat([token_ids,new_token.unsqueeze(0)],dim=1)\n",
        "            token_ids = torch.cat([token_ids,new_token.unsqueeze(1)],dim=1)\n",
        "\n",
        "    # return decode(token_ids.tolist())\n",
        "        return token_ids\n",
        "        # pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP8430nWKbZ6"
      },
      "source": [
        "Train your `TransformerLM` for `LARGE_ITERS` iterations and plot the loss curve. You may want to change the learning rate.\n",
        "\n",
        "We used a learning rate of `1e-4` and got to a final train loss of around 1.4 in around 15 minutes of training on a T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73du7-sWLH5c"
      },
      "outputs": [],
      "source": [
        "# conditional generation from the model\n",
        "\n",
        "context1 = \"\"\"ROMEO:\n",
        "He jests at scars that never felt a wound.\n",
        "But, soft! what light through yonder window breaks?\n",
        "It is the east, and Juliet is the sun.\n",
        "Arise, fair sun, and kill the envious moon,\n",
        "Who is already sick and pale with grief,\n",
        "That thou her maid art far more fair than she:\n",
        "Be not her maid, \"\"\"\n",
        "\n",
        "# context1_tokens = torch.tensor(encode(context1), device=device).reshape(1, -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Izr1wTOjzlo"
      },
      "source": [
        "## Part 2: Mini-Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lF3jFrQj1f4"
      },
      "source": [
        "Quick recap: So far we have\n",
        "\n",
        "1. Preprocessed the Shakespeare dataset by encoding individual characters into integer tokens.\n",
        "2. Implemented single headed attention and then further generalized to multiheaded attention. We further combined multiheaded attention with deep learning to create the transformer architecture.\n",
        "3. Trained our transformer and generated output that looks to be in the style of Shakespeare.\n",
        "\n",
        "Up to this point, the performance of our simple language model has clearly made a lot of progress. We can see that our model has learned to generate text that is close to the style of Shakespeare, although there are still many quirks and room for improvement.\n",
        "\n",
        "### Project Outline\n",
        "\n",
        "Find some area of possible improvement.\n",
        "We interpret \"improvement\" quite loosely, but please state precisely why your proposed innovation might improve the model, and provide evidence that it does (or does not!) improve.\n",
        "For your idea, **formulate a hypothesis** for why this change should result in a better model. **Implement your changes** and **report any findings**.\n",
        "\n",
        "_Notes_: As this assignment is being treated as a project, you should expect training to take longer than previous assignments. However, please use your judgement to decide what is reasonable. We will not expect you to run training procedures that take more than 2 hours on the free Google Colab computing resources and we certainly do not expect you to acquire additional compute. The proposed improvements should not solely rely on increased computing demands.\n",
        "\n",
        "_Hints_: There are many aspects to assessing a model. For example, not only is quality of generated text important, it is also of interest to reduce costs associated with training.\n",
        "\n",
        "### Deliverables\n",
        "\n",
        "In addition to a pdf of your python notebook, the submission for this project will be a written report no more than 4 pages in length using the [NeurIPS LaTex template](https://neurips.cc/Conferences/2023/PaperInformation/StyleFiles). Your report should include detailed analysis of the hypotheses you chose to test along with any conclusions.\n",
        "\n",
        "The page limit for the report does not include bibliography or appendices. Make sure to keep the \"ready for submission\" option to help us grade anonymously. Your writeup should also contain a link to any code used to generate the project so that we can reference it while grading (Google Drive folder with colab notebooks or Github repo are both fine). You should have at least one plot in your main text (which is capped at 4 pages)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OfgjLjDc4lq"
      },
      "source": [
        "### TransformerLM_2 is has Dropout added to the output of the attention and mlp layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Xu79GPJk8uXY"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock_2(nn.Module):\n",
        "    \"\"\" Transformer block: communication across sequence length, followed by communication across embedding space\n",
        "        Uses multi-headed attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6,dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(embed_size)\n",
        "        self.ln2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        # TODO: your code below\n",
        "        self.feed_forward = FeedForward(embed_size) # acts along rows x_t^(m+1) = mlp(y_t^(m))\n",
        "        self.atten_heads = MultiHeadAttention(context_window_size, num_heads, embed_size//num_heads, embed_size) # as before\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "        # for dropout\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout) # takes a dropout rate arg\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.atten_heads(self.ln1(x)) # communication over sequence length y_t^(m) = atten_heads(x_t^(m))\n",
        "        x = x + self.feed_forward(self.ln2(x)) # communication across embedding space x_t^(m+1) = mlp(y_t^(m))\n",
        "\n",
        "\n",
        "\n",
        "        # for adding dropout\n",
        "        # x = x + self.dropout(self.atten_heads(self.ln1(x))) # communication over sequence length y_t^(m) = atten_heads(x_t^(m))\n",
        "        # x = x + self.dropout(self.feed_forward(self.ln2(x))) # communication across embedding space x_t^(m+1) = mlp(y_t^(m))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "mKD_4-8K8rgw"
      },
      "outputs": [],
      "source": [
        "class TransformerLM_2(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6, n_layers=6):\n",
        "        \"\"\"\n",
        "          Args:\n",
        "              vocab_size: int, number of tokens in the vocabulary (V)\n",
        "              context_window_size: int, size of the context window (T)\n",
        "              embed_size: int, embedding size (D)\n",
        "              num_heads: int, number of heads (H)\n",
        "              n_layers: int, number of layers (M)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
        "        self.position_embedding_table = nn.Embedding(context_window_size, embed_size)\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            TransformerBlock_2(vocab_size,\n",
        "                             context_window_size,\n",
        "                             embed_size=embed_size,\n",
        "                             num_heads=num_heads)\n",
        "            for _ in range(n_layers)]) # chains output of one block to input of next. Output of self.blocks is x_t^M\n",
        "\n",
        "        # final layer norm\n",
        "        self.ln_f = nn.LayerNorm(embed_size)\n",
        "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # good initialization\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, token_ids, targets=None):\n",
        "        \"\"\"\n",
        "        Agrgs:\n",
        "            token_ids: tensor of integers, provides the contet, shape (B, T)\n",
        "            targets: tensor of integers, provides the tokens we are preidcitng, shape (B, T)\n",
        "        \"\"\"\n",
        "        B, T = token_ids.shape\n",
        "\n",
        "        # token_ids and targets are both (B, T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(token_ids) # (B, T, D)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, D)\n",
        "        x = tok_emb + pos_emb # (B, T, D)\n",
        "\n",
        "        # TODO: your code below\n",
        "        x = self.blocks(x) # instead of just x=self.attention_head(x), x is processed through a sequence of transformer blocks, each first processed by \"self attn\" then by mlp.\n",
        "        # this x is x_t^M in Scott's notes\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss_fcn = torch.nn.CrossEntropyLoss()\n",
        "        loss = loss_fcn(logits.reshape(-1, self.vocab_size), targets.reshape(-1)) # as before\n",
        "\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, token_ids, max_new_tokens):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            token_ids: tensor of integers forming the context, shape (B, T)\n",
        "            max_new_tokens: int, max number of tokens to generate\n",
        "\n",
        "        \"\"\"\n",
        "        # if token_ids.shape[0] ==1:\n",
        "        #     token_ids = token_ids.unsqueeze(0)\n",
        "        #     token_ids = token_ids.unsqueeze(0)\n",
        "\n",
        "        for i in range(max_new_tokens):\n",
        "            B, T = token_ids.shape # (batch size, length)\n",
        "            t = min(CONTEXT_WINDOW_SIZE,T)\n",
        "            tok_emb = self.token_embedding_table(token_ids[:,-t:]).reshape(B,t,-1) # (B,T,D)\n",
        "            pos_emb = self.position_embedding_table(torch.arange(t, device=device)) # (T,D)\n",
        "            x = tok_emb + pos_emb # (B,T,D)\n",
        "            x = self.blocks(x) # (B,T,D)\n",
        "            x= self.ln_f(x)\n",
        "            logits = self.lm_head(x) # (B,T,V)\n",
        "            # if len(logits.shape) == 2:\n",
        "            #     new_token = torch.argmax(logits[-1,:])\n",
        "            # else:\n",
        "            #     new_token = torch.argmax(logits[:,-1],dim=1)\n",
        "            # new_token = torch.argmax(logits[:,-1],dim=1) # best token for each batch\n",
        "            new_token = torch.distributions.Categorical(logits=logits[:,-1]).sample()\n",
        "            # token_ids = torch.cat([token_ids,new_token.unsqueeze(0)],dim=1)\n",
        "            token_ids = torch.cat([token_ids,new_token.unsqueeze(1)],dim=1)\n",
        "\n",
        "    # return decode(token_ids.tolist())\n",
        "        return token_ids\n",
        "        # pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hy_A0NbZ8yPi",
        "outputId": "6e3e578a-d1d2-408a-ce62-3e71665710a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-125e4dcc4c7a>:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "  0%|          | 0/4000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0 batch size 32\n",
            "step 0: train loss 4.3019, val loss 4.2964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-125e4dcc4c7a>:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "  5%|▌         | 200/4000 [00:55<08:42,  7.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 200 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 201/4000 [01:18<7:33:01,  7.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: train loss 2.5138, val loss 2.5184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 400/4000 [01:47<08:24,  7.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 400 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 401/4000 [02:11<7:33:21,  7.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: train loss 2.3536, val loss 2.3647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▌        | 600/4000 [02:39<07:48,  7.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 600 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 601/4000 [03:03<6:58:55,  7.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: train loss 2.0914, val loss 2.1333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 800/4000 [03:31<07:24,  7.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 800 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 801/4000 [03:56<6:35:07,  7.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: train loss 1.8992, val loss 1.9926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 1000/4000 [04:23<06:53,  7.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1000 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 1001/4000 [04:48<6:10:16,  7.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1000: train loss 1.7693, val loss 1.9050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 1200/4000 [05:15<06:26,  7.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1200 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 1201/4000 [05:40<5:45:24,  7.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1200: train loss 1.6759, val loss 1.8464\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|███▌      | 1400/4000 [06:07<06:05,  7.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1400 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▌      | 1401/4000 [06:32<5:20:47,  7.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1400: train loss 1.6156, val loss 1.7857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 1600/4000 [06:59<05:30,  7.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1600 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 1601/4000 [07:24<4:56:14,  7.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1600: train loss 1.5562, val loss 1.7306\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▌     | 1800/4000 [07:51<05:04,  7.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1800 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 1801/4000 [08:16<4:31:28,  7.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1800: train loss 1.5173, val loss 1.7000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 2000/4000 [08:43<04:41,  7.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 2000 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 2001/4000 [09:08<4:06:54,  7.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 2000: train loss 1.4725, val loss 1.6705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|█████▌    | 2200/4000 [09:35<04:08,  7.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 2200 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▌    | 2201/4000 [10:00<3:42:03,  7.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 2200: train loss 1.4500, val loss 1.6604\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 2400/4000 [10:27<03:40,  7.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 2400 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 2401/4000 [10:52<3:17:28,  7.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 2400: train loss 1.4172, val loss 1.6371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▌   | 2600/4000 [11:19<03:14,  7.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 2600 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 2601/4000 [11:44<2:52:53,  7.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 2600: train loss 1.3916, val loss 1.6149\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 2800/4000 [12:11<02:50,  7.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 2800 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 2801/4000 [12:36<2:28:07,  7.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 2800: train loss 1.3705, val loss 1.6068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 3000/4000 [13:03<02:17,  7.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 3000 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 3001/4000 [13:28<2:03:20,  7.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 3000: train loss 1.3548, val loss 1.5872\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 3200/4000 [13:55<01:50,  7.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 3200 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 3201/4000 [14:20<1:38:39,  7.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 3200: train loss 1.3345, val loss 1.5939\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 85%|████████▌ | 3400/4000 [14:47<01:25,  7.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 3400 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▌ | 3401/4000 [15:12<1:14:01,  7.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 3400: train loss 1.3241, val loss 1.5827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 3600/4000 [15:39<00:55,  7.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 3600 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 3601/4000 [16:04<49:16,  7.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 3600: train loss 1.3052, val loss 1.5845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▌| 3800/4000 [16:31<00:27,  7.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 3800 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▌| 3801/4000 [16:56<24:34,  7.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 3800: train loss 1.2890, val loss 1.5737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 3999/4000 [17:23<00:00,  7.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 3999 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4000/4000 [17:47<00:00,  3.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 3999: train loss 1.2752, val loss 1.5648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "LARGE_ITERS = 4000\n",
        "\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "scaler = GradScaler()\n",
        "\n",
        "trans = TransformerLM_2(vocab_size, CONTEXT_WINDOW_SIZE)\n",
        "tlm = trans.to(device)\n",
        "learning_rate = 1e-4\n",
        "# TODO, your code below\n",
        "\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "torch.manual_seed(760)\n",
        "\n",
        "optimizer = torch.optim.AdamW(trans.parameters(), lr=learning_rate)\n",
        "# scheduler = CosineAnnealingLR(optimizer, T_max=LARGE_ITERS, eta_min=1e-6)\n",
        "\n",
        "eval_interval = 200\n",
        "\n",
        "# batch params\n",
        "batch_start = 16\n",
        "batch_max = 128\n",
        "batch_growth = LARGE_ITERS//4\n",
        "\n",
        "\n",
        "loss_list = []\n",
        "\n",
        "for it in tqdm(range(LARGE_ITERS)):\n",
        "    #update the batch size\n",
        "    # batch_size = min(batch_start *(2**(it // batch_growth)), batch_max)\n",
        "    batch_size = 32\n",
        "\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if it % eval_interval == 0 or it == LARGE_ITERS - 1:\n",
        "        print(f\"iteration {it}\", f\"batch size {batch_size}\")\n",
        "        losses = estimate_loss(tlm, EVAL_ITERS, CONTEXT_WINDOW_SIZE, device)\n",
        "        print(f\"step {it}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train', CONTEXT_WINDOW_SIZE, device, batch_size=batch_size)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # evaluate the loss\n",
        "    with autocast():\n",
        "      logits, loss = tlm(xb, yb)\n",
        "    # logits, loss = tlm(xb, yb)\n",
        "    scaler.scale(loss).backward()\n",
        "\n",
        "    # Gradient clipping (optional for stability)\n",
        "    # torch.nn.utils.clip_grad_norm_(tlm.parameters(), 1)\n",
        "    loss_list.append(loss.detach().item())\n",
        "\n",
        "    # Step optimizer and scheduler with scaler\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # scheduler.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "rpYbPciURnrc",
        "outputId": "f1fb441c-82f5-4485-bf2d-c8e3ef694bd4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Loss')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAYYlJREFUeJzt3XdcVfX/B/DXZV1AuEwZKoIKgshQceGkxIFWomWGlqOyTC39NTX7OitMs2+m5civI8ssLWcuHDhx4AYVN6AyRGXLBe49vz+Io1c23sXl9Xw87iPuOZ9z7vtzL3bffKZEEAQBRERERAbCSNcBEBEREakTkxsiIiIyKExuiIiIyKAwuSEiIiKDwuSGiIiIDAqTGyIiIjIoTG6IiIjIoDC5ISIiIoPC5IaIiIgMCpMboifMmDEDEolE12FU26hRo+Dh4VGra+taXQ3Rzp070aZNG5ibm0MikSAzM1PXIekVDw8PjBo1SiP3vnXrFiQSCb799luN3J90i8kNacWqVasgkUgQGxur61C06u7du5gxYwbOnj2r61DqjOvXr2PYsGFwcnKChYUFvLy8MHXq1ArLl35JVedx69Yt7VWkCvfv38err74KCwsL/Pjjj1izZg0aNGig67A0Kjo6usxnYm9vj86dO+O3337TdXhkQEx0HQCRPvniiy8wefJktd3v7t27mDlzJjw8PNCmTRu13bfUzz//DKVSWatr1V1XdTh79ixCQkLQuHFjfPTRR3BwcEBSUhKSk5MrvKZhw4ZYs2aNyrH58+fj9u3b+O9//1umrL44efIkcnJyMHv2bISGhuo6HK364IMP0KFDBwAlSd4ff/yB119/HZmZmRg/fryOoyNDwOSG6AkmJiYwMdHdP4v8/HxYWlpWu7ypqWmtX0vXdX2aUqnEG2+8AR8fH+zfvx8WFhbVuq5BgwZ4/fXXVY6tW7cODx8+LHP8SYIgoKCgoNqvo27p6ekAAFtbW7XdMy8vT+etP9WJoXv37njllVfE5++99x6aN2+OtWvXMrkhtWC3FOmVM2fOICwsDDKZDFZWVujVqxeOHTumUqaoqAgzZ86El5cXzM3N4eDggG7duiEqKkosk5qaitGjR6NJkyaQSqVwdXXFwIEDq+yWKG8cikQiwYQJE7Bp0yb4+flBKpWidevW2LlzZ6X3io6OFv86HT16tNgMv2rVKgBASEgI/Pz8cOrUKfTo0QOWlpb4/PPPAQCbN2/GgAED0KhRI0ilUrRo0QKzZ8+GQqFQeY2nx9w8OY5g2bJlaNGiBaRSKTp06ICTJ0+qta7R0dFo3749zM3N0aJFCyxduvSZxvHs3r0bcXFxmD59OiwsLJCfn1+mvs/Cw8MDL7zwAnbt2oX27dvDwsICS5cuBQCsXLkSzz//PJycnCCVSuHr64vFixdXeI/Dhw+jY8eOMDc3R/PmzfHLL7+olKvqdzQkJAQjR44EAHTo0AESiURlbMn69esRFBQECwsLODo64vXXX8edO3dUXmPUqFGwsrLC9evX0b9/f1hbW2P48OEAHn+O69evh6+vLywsLBAcHIwLFy4AAJYuXQpPT0+Ym5sjJCSk3H8Xx48fR79+/WBjYwNLS0v07NkTR44cUSlT+nlfvHgRw4YNg52dHbp161aDT6WEmZkZ7OzsqpVs37hxA0OGDIG9vT0sLS3RuXNn/PPPP2XKFRQUYMaMGWjZsiXMzc3h6uqKwYMH4/r16xXeWxAEvPPOOzAzM8Pff/8NoHr/vyH9oz9/tlG9Fx8fj+7du0Mmk+HTTz+Fqakpli5dipCQEBw4cACdOnUCUPI/1MjISLz99tvo2LEjsrOzERsbi9OnT6N3794AgJdffhnx8fF4//334eHhgfT0dERFRSEpKalWA3APHz6Mv//+G+PGjYO1tTV++OEHvPzyy0hKSoKDg0O517Rq1QqzZs3CtGnT8M4776B79+4AgC5duohl7t+/j7CwMLz22mt4/fXX4ezsDKBkjJKVlRU+/PBDWFlZYd++fZg2bRqys7Mxb968KuNdu3YtcnJy8O6770IikWDu3LkYPHgwbty4UWVrT3XqeubMGfTr1w+urq6YOXMmFAoFZs2a9UzdPnv27AEASKVStG/fHqdOnYKZmRkGDRqEn376Cfb29rW+d6mEhARERETg3XffxZgxY+Dt7Q0AWLx4MVq3bo2XXnoJJiYm2Lp1K8aNGwelUlmmJeHatWt45ZVX8NZbb2HkyJFYsWIFRo0ahaCgILRu3RpA1b+jU6dOhbe3N5YtW4ZZs2ahWbNmaNGiBYCSz3706NHo0KEDIiMjkZaWhgULFuDIkSM4c+aMSktPcXEx+vbti27duuHbb79VafU7dOgQtmzZIsYfGRmJF154AZ9++il++uknjBs3Dg8fPsTcuXPx5ptvYt++feK1+/btQ1hYGIKCgjB9+nQYGRmJCeChQ4fQsWNHlfdkyJAh8PLywtdffw1BEKr8HHJycpCRkQEAePDgAdauXYu4uDj873//q/S6tLQ0dOnSBfn5+fjggw/g4OCA1atX46WXXsKGDRswaNAgAIBCocALL7yAvXv34rXXXsPEiRORk5ODqKgoxMXFie/1kxQKBd5880388ccf2LhxIwYMGFCtz5L0lECkBStXrhQACCdPnqywTHh4uGBmZiZcv35dPHb37l3B2tpa6NGjh3gsMDBQGDBgQIX3efjwoQBAmDdvXo3jnD59uvD0PwsAgpmZmXDt2jXx2Llz5wQAwsKFCyu938mTJwUAwsqVK8uc69mzpwBAWLJkSZlz+fn5ZY69++67gqWlpVBQUCAeGzlypODu7i4+v3nzpgBAcHBwEB48eCAe37x5swBA2Lp1q1rq+uKLLwqWlpbCnTt3xGNXr14VTExMytyzul566SUx9uHDhwsbNmwQ/vOf/wgmJiZCly5dBKVSWe17DRgwQOV9EQRBcHd3FwAIO3fuLFO+vPe7b9++QvPmzcu9x8GDB8Vj6enpglQqFT766CPxWFW/o4JQ/r+JwsJCwcnJSfDz8xMePXokHt+2bZsAQJg2bZp4bOTIkQIAYfLkyWXuDUCQSqXCzZs3xWNLly4VAAguLi5Cdna2eHzKlCkCALGsUqkUvLy8hL59+6q85/n5+UKzZs2E3r17i8dKf4ciIiIqrWup/fv3CwDKPIyMjISvvvqqTHl3d3dh5MiR4vNJkyYJAIRDhw6Jx3JycoRmzZoJHh4egkKhEARBEFasWCEAEL777rsy9yytU+m/lXnz5glFRUXC0KFDBQsLC2HXrl0q5avzWZL+YbcU6QWFQoHdu3cjPDwczZs3F4+7urpi2LBhOHz4MLKzswGUjFGIj4/H1atXy72XhYUFzMzMEB0djYcPH6olvtDQUJW/9gICAiCTyXDjxo1nuq9UKsXo0aPLHH9yHEjpX7ndu3dHfn4+Ll++XOV9hw4dCjs7O/F5aatRdeKtqq4KhQJ79uxBeHg4GjVqJJbz9PREWFhYlfevSG5uLoCSbppff/0VL7/8MmbNmoXZs2fj6NGj2Lt3b63vXapZs2bo27dvmeNPvt9ZWVnIyMhAz549cePGDWRlZamU9fX1Fd9PoGSQsre3t8p7W9XvaEViY2ORnp6OcePGwdzcXDw+YMAA+Pj4lNv98t5775V7r169eqm0Upa2fL788suwtrYuc7w0/rNnz+Lq1asYNmwY7t+/j4yMDGRkZCAvLw+9evXCwYMHywxiHzt2bI3qOW3aNERFRSEqKgp//PEHIiIiMHXqVCxYsKDS67Zv346OHTuqdH1ZWVnhnXfewa1bt3Dx4kUAwF9//QVHR0e8//77Ze7xdLdpYWEhhgwZgm3btmH79u3o06ePyvnafpakW0xuSC/cu3cP+fn5YjfBk1q1agWlUinOmJk1axYyMzPRsmVL+Pv745NPPsH58+fF8lKpFN988w127NgBZ2dn9OjRA3PnzkVqamqt42vatGmZY3Z2ds+cPDVu3BhmZmZljsfHx2PQoEGwsbGBTCZDw4YNxcGxT3/ZVife0kSnOvFWVdf09HQ8evQInp6eZcqVd6y6ShOMiIgIlePDhg0DABw9erTW9y7VrFmzco8fOXIEoaGhaNCgAWxtbdGwYUNx/NPT73d1fheq+h2tSGJiIgCU++/Ax8dHPF/KxMQETZo0KfdeT8dpY2MDAHBzcyv3eGn8pV/iI0eORMOGDVUey5cvh1wuL/OeVPS+VsTf3x+hoaEIDQ3Fq6++il9//RUvvPACJk+ejHv37lV4XWJiYoX/jyg9D5QsJ+Dt7V2tMTyRkZHYtGkTNmzYgJCQkDLna/tZkm4xuaE6p0ePHrh+/TpWrFgBPz8/LF++HO3atcPy5cvFMpMmTcKVK1cQGRkJc3Nz/Oc//0GrVq1w5syZWr2msbFxuceFaowvqEx5M3UyMzPRs2dPnDt3DrNmzcLWrVsRFRWFb775BgCqNfX7WeLVVF2rUtoKVDruqJSTkxOA6iVmVSnv/b5+/Tp69eqFjIwMfPfdd/jnn38QFRWF//u//wNQ9v2uzvtTnd9RdZBKpTAyKv9/4xXFWVX8pfWdN2+e2Lry9MPKykrlWnXMOOvVqxcKCgpw4sSJZ75XTfTt2xcNGjTA3LlzUVBQUOa8tj5LUi8mN6QXGjZsCEtLSyQkJJQ5d/nyZRgZGan8xWlvb4/Ro0fj999/R3JyMgICAjBjxgyV61q0aIGPPvpInIVTWFiI+fPna7oqKmozcyg6Ohr379/HqlWrMHHiRLzwwgsIDQ1V6WbSJScnJ5ibm+PatWtlzpV3rLqCgoIAoMysoLt37wLQ3Bo1W7duhVwux5YtW/Duu++if//+CA0NfeYv7Or8jj7N3d0dAMr9d5CQkCCe16TSLkmZTCa2rjz9eJYlCCpSXFwM4HH3ZHnc3d0r/H9E6XmgpA4JCQkoKiqq8nU7d+6MTZs24ejRoxgyZIgYx5Nq81mSbjG5Ib1gbGyMPn36YPPmzSrTUtPS0rB27Vp069YNMpkMQMkMoydZWVnB09MTcrkcQMlaMU//BdaiRQtYW1uLZbSldL2PmiyrX/qX9ZMtAYWFhfjpp5/UGlttGRsbIzQ0FJs2bRITD6AksdmxY0et7ztw4EBIpVKsXLlSpbWk9C9kTc1MKe/9zsrKwsqVK2t9z6p+RyvSvn17ODk5YcmSJSpld+zYgUuXLokzeDQpKCgILVq0wLfffltuolFZt9Gz2LZtGwAgMDCwwjL9+/fHiRMnEBMTIx7Ly8vDsmXL4OHhAV9fXwAl44oyMjKwaNGiMvcorwUyNDQU69atw86dO/HGG2+o/P7V9rMk3eJUcNKqFStWlLtmysSJE/Hll18iKioK3bp1w7hx42BiYoKlS5dCLpdj7ty5YllfX1+EhIQgKCgI9vb2iI2NxYYNGzBhwgQAwJUrV9CrVy+8+uqr8PX1hYmJCTZu3Ii0tDS89tprWqsrUJJU2draYsmSJbC2tkaDBg3QqVOnSscodOnSBXZ2dhg5ciQ++OADSCQSrFmzRuPdQjUxY8YM7N69G127dsV7770HhUKBRYsWwc/Pr8xWEzNmzMDMmTOxf//+csc0lHJxccHUqVMxbdo09OvXD+Hh4Th37hx+/vlnREREiGsGqVufPn1gZmaGF198Ee+++y5yc3Px888/w8nJCSkpKbW6Z1W/oxUxNTXFN998g9GjR6Nnz56IiIgQp4J7eHiIXWWaZGRkhOXLlyMsLAytW7fG6NGj0bhxY9y5cwf79++HTCbD1q1bn+k1Dh06JP4B8uDBA2zZsgUHDhzAa6+9Bh8fnwqvmzx5Mn7//XeEhYXhgw8+gL29PVavXo2bN2/ir7/+ErvoRowYgV9++QUffvghTpw4ge7duyMvLw979uzBuHHjMHDgwDL3Dg8Px8qVKzFixAjIZDJxDaTafpakW0xuSKvKWxgNKFmQrHXr1jh06BCmTJmCyMhIKJVKdOrUCb/++qs4owMoWbp9y5Yt2L17N+RyOdzd3fHll1/ik08+AVAyYDIiIgJ79+7FmjVrYGJiAh8fH/z55594+eWXtVLPUqampli9ejWmTJmCsWPHori4GCtXrqw0uXFwcMC2bdvw0Ucf4YsvvoCdnR1ef/119OrVq9yZProQFBSEHTt24OOPP8Z//vMfuLm5YdasWbh06VKZ2Vy5ubmQSCRwcXGp8r6l9V24cCEmTZqkkvBoire3NzZs2IAvvvgCH3/8MVxcXPDee++hYcOGePPNN2t1z6p+RyszatQoWFpaYs6cOfjss8/QoEEDDBo0CN98841aVzOuTEhICGJiYjB79mwsWrQIubm5cHFxQadOnfDuu+8+8/1/+OEH8WczMzM0b94cX331VZXvj7OzM44ePYrPPvsMCxcuREFBAQICArB161aVVi1jY2Ns374dX331FdauXYu//vpLXHzP39+/wvu//vrryMnJwbhx4yCTyTBv3rxn+ixJdySCPv05SER1Wnh4eJlpsx07doS7uzvWr1+vw8iIqD7hmBsiqpVHjx6pPL969Sq2b9+u0vWUnZ0tzvoiItIWttwQUa24urpi1KhRaN68ORITE7F48WLI5XKcOXMGXl5eug6PiOoxjrkholrp168ffv/9d6SmpkIqlSI4OBhff/01Exsi0jm23BAREZFB4ZgbIiIiMihMboiIiMig1LsxN0qlEnfv3oW1tXWtlsYnIiIi7RMEATk5OWjUqFGFe6qVqnfJzd27d8vsiktERER1Q3JyMpo0aVJpmXqX3FhbWwMoeXNK9yoiIiIi/ZadnQ03Nzfxe7wy9S65Ke2KkslkTG6IiIjqmOoMKeGAYiIiIjIoTG6IiIjIoDC5ISIiIoPC5IaIiIgMCpMbIiIiMihMboiIiMigMLkhIiIig8LkhoiIiAwKkxsiIiIyKExuiIiIyKAwuSEiIiKDwuSGiIiIDEq92zhTU+TFCtzLkcPEyAguNua6DoeIiKjeYsuNmsTdyUa3b/bj1aUxug6FiIioXmNyo2YCBF2HQEREVK8xuVETiaTkvwJzGyIiIp1icqMmEl0HQERERACY3KgdW26IiIh0i8mNmkgkbLshIiLSB0xu1ISpDRERkX5gcqNmAvuliIiIdIrJjZqwV4qIiEg/MLlRE8m/HVNstyEiItItJjdqxl4pIiIi3WJyoybsliIiItIPTG7UjNsvEBER6RaTGzVjtxQREZFuMblRE3ZLERER6QcmN2rGhhsiIiLdYnKjJuJUcGY3REREOsXkRk3YLUVERKQfmNyoHZtuiIiIdInJjZqUttywW4qIiEi3mNyoiYT7ghMREekFJjdqxoYbIiIi3WJyoyaPu6WY3hAREekSkxs1YacUERGRfmByo2ZstyEiItItJjdqwnVuiIiI9AOTG7XhCsVERET6gMmNmnFAMRERkW4xuVETdksRERHpByY3alKa27DdhoiISLf0JrmZM2cOJBIJJk2aVGm59evXw8fHB+bm5vD398f27du1E2B1MbshIiLSKb1Ibk6ePImlS5ciICCg0nJHjx5FREQE3nrrLZw5cwbh4eEIDw9HXFycliKtmIT9UkRERHpB58lNbm4uhg8fjp9//hl2dnaVll2wYAH69euHTz75BK1atcLs2bPRrl07LFq0SEvRVozdUkRERPpB58nN+PHjMWDAAISGhlZZNiYmpky5vn37IiYmpsJr5HI5srOzVR6axNlSREREumWiyxdft24dTp8+jZMnT1arfGpqKpydnVWOOTs7IzU1tcJrIiMjMXPmzGeKszrYK0VERKQfdNZyk5ycjIkTJ+K3336Dubm5xl5nypQpyMrKEh/JyckaeR1J6SJ+Grk7ERERVZfOWm5OnTqF9PR0tGvXTjymUChw8OBBLFq0CHK5HMbGxirXuLi4IC0tTeVYWloaXFxcKnwdqVQKqVSq3uArwV4pIiIi3dJZy02vXr1w4cIFnD17Vny0b98ew4cPx9mzZ8skNgAQHByMvXv3qhyLiopCcHCwtsKuELuliIiI9IPOWm6sra3h5+encqxBgwZwcHAQj48YMQKNGzdGZGQkAGDixIno2bMn5s+fjwEDBmDdunWIjY3FsmXLtB5/RQR2TBEREemUzmdLVSYpKQkpKSni8y5dumDt2rVYtmwZAgMDsWHDBmzatKlMkqRL7JYiIiLSLZ3OlnpadHR0pc8BYMiQIRgyZIh2AqoBdksRERHpB71uualLSlcoZsMNERGRbjG5UTdmN0RERDrF5EZN2CtFRESkH5jcqEnpmBvOliIiItItJjdERERkUJjcqIm4/QIbboiIiHSKyY2aPO6WIiIiIl1ickNEREQGhcmNmpTOlhLYL0VERKRTTG7Uhd1SREREeoHJDRERERkUJjdqwtlSRERE+oHJjZpw40wiIiL9wOSGiIiIDAqTGzV5suGGM6aIiIh0h8mNmkie6JdibkNERKQ7TG6IiIjIoDC5UROVbimdRUFERERMbtTkydlSHHNDRESkO0xuiIiIyKAwuVETyRMdU2y3ISIi0h0mN+qi0i2luzCIiIjqOyY3REREZFCY3KiJyoBidkwRERHpDJMbNeHWUkRERPqByY0GcMwNERGR7jC5URMJtwUnIiLSC0xu1ISpDRERkX5gcqMB7JYiIiLSHSY3asLZUkRERPqByY2aSNgxRUREpBeY3GgAu6WIiIh0h8mNmqh2SxEREZGuMLkhIiIig8LkRgME9ksRERHpDJMbNWG3FBERkX5gckNEREQGhcmNmjw5FZy9UkRERLrD5EZNVLaWYnJDRESkM0xuiIiIyKAwuVET1YYbNt0QERHpCpMbNZFIOOaGiIhIHzC5ISIiIoPC5EZNOJ6YiIhIP+g0uVm8eDECAgIgk8kgk8kQHByMHTt2VFh+1apVkEgkKg9zc3MtRlwxlUX82C9FRESkMya6fPEmTZpgzpw58PLygiAIWL16NQYOHIgzZ86gdevW5V4jk8mQkJAgPpeozMEmIiKi+k6nyc2LL76o8vyrr77C4sWLcezYsQqTG4lEAhcXF22EVyMqA4p1GAcREVF9pzdjbhQKBdatW4e8vDwEBwdXWC43Nxfu7u5wc3PDwIEDER8fX+l95XI5srOzVR5ERERkuHSe3Fy4cAFWVlaQSqUYO3YsNm7cCF9f33LLent7Y8WKFdi8eTN+/fVXKJVKdOnSBbdv367w/pGRkbCxsREfbm5umqoKTIxKWm+KFWy7ISIi0hWJoOPRr4WFhUhKSkJWVhY2bNiA5cuX48CBAxUmOE8qKipCq1atEBERgdmzZ5dbRi6XQy6Xi8+zs7Ph5uaGrKwsyGQytdUDAHyn7UR+oQIHP3kOTR0s1XpvIiKi+iw7Oxs2NjbV+v7W6ZgbADAzM4OnpycAICgoCCdPnsSCBQuwdOnSKq81NTVF27Ztce3atQrLSKVSSKVStcVbGamJEfILFZAXK7TyekRERFSWzrulnqZUKlVaWiqjUChw4cIFuLq6ajiq6pGaGAMA5MVKHUdCRERUf+m05WbKlCkICwtD06ZNkZOTg7Vr1yI6Ohq7du0CAIwYMQKNGzdGZGQkAGDWrFno3LkzPD09kZmZiXnz5iExMRFvv/22LqshkpqW5IpMboiIiHRHp8lNeno6RowYgZSUFNjY2CAgIAC7du1C7969AQBJSUkwMnrcuPTw4UOMGTMGqampsLOzQ1BQEI4ePVqt8TnaYGZcmtywW4qIiEhXdD6gWNtqMiCppl5YeAhxd7KxcnQHPOftpNZ7ExER1Wc1+f7WuzE3dZk45qaILTdERES6wuRGjSzNSpKbPDmTGyIiIl1hcqNGVtKSIUx5hcU6joSIiKj+YnKjRpZmJcnN36fv6DgSIiKi+ovJjRrlyosAAGeTM1HPxmkTERHpDSY3aqR8Ip8pKOJaN0RERLrA5EaNbCxMxZ//On0byw/d0GE0RERE9ZPO95YyJDLzx8nNF5viAABdWjjCt5F619MhIiKiirHlRo0aWpfdoDM9p0AHkRAREdVfTG7UyNq8bEPYB7+f0UEkRERE9ReTGzV6uV2TMseyC7jmDRERkTYxuVEjCzNjcSG/J/1+IkkH0RAREdVPTG7UbPWbHcscm/L3BeRz1WIiIiKtYHKjZkHudviwd8syx7nfFBERkXYwudGA6IT0MscEcMViIiIibWByowG37ueXOVasYHJDRESkDUxuNOCjPmW7pYoU3I6BiIhIG5jcaECnZvZljiU9KNuaQ0REROrH5EYDjCSSMsfGrjmlg0iIiIjqHyY3GmBsVDa5ySvkbCkiIiJtYHKjAU3sLOHpZAX/xja6DoWIiKjeYXKjAcZGEuya1AObx3dVOc6F/IiIiDSPyY2GGBtJYPRU95S8iDOmiIiINI3JjRYVK7nWDRERkaYxudEiBZMbIiIijWNyo2GHPn1O/FkhMLkhIiLSNCY3GuZmbyn+nFTOtgxERESkXkxutGjm1nhdh0BERGTwmNxoUU4Bp4ITERFpGpMbLSpnVwYiIiJSMyY3WnT74SNdh0BERGTwmNwQERGRQWFyQ0RERAaFyQ0REREZFCY3REREZFCY3BAREZFBYXKjBWN7tgAAtHKV6TgSIiIiw8fkRguaN2wAAHCRSXUcCRERkeFjcqMFxv+u3rc/4R53BiciItIwJjdakPWoSPx576U0HUZCRERk+JjcaMGTe0o9KlLoMBIiIiLDx+RGC0Z19RB/NjHiW05ERKRJ/KbVAhsLU/FnRyszHUZCRERk+HSa3CxevBgBAQGQyWSQyWQIDg7Gjh07Kr1m/fr18PHxgbm5Ofz9/bF9+3YtRftsvJysAIADiomIiDRMp8lNkyZNMGfOHJw6dQqxsbF4/vnnMXDgQMTHx5db/ujRo4iIiMBbb72FM2fOIDw8HOHh4YiLi9Ny5DVnbFQyY6qYyQ0REZFGSQRB0KtvW3t7e8ybNw9vvfVWmXNDhw5FXl4etm3bJh7r3Lkz2rRpgyVLllTr/tnZ2bCxsUFWVhZkMu0tqtfv+4O4nJqDlaM74DlvJ629LhERkSGoyfe33oy5USgUWLduHfLy8hAcHFxumZiYGISGhqoc69u3L2JiYiq8r1wuR3Z2tspDFwoVSgDA8RsPdPL6RERE9YXOk5sLFy7AysoKUqkUY8eOxcaNG+Hr61tu2dTUVDg7O6scc3Z2RmpqaoX3j4yMhI2Njfhwc3NTa/zVdeNeHgDg8LV7Onl9IiKi+kLnyY23tzfOnj2L48eP47333sPIkSNx8eJFtd1/ypQpyMrKEh/Jyclqu3dNNHcs2YIh7o5uWo6IiIjqCxNdB2BmZgZPT08AQFBQEE6ePIkFCxZg6dKlZcq6uLggLU11hd+0tDS4uLhUeH+pVAqpVPd7Ork7WOJGRp6uwyAiIjJ4Om+5eZpSqYRcLi/3XHBwMPbu3atyLCoqqsIxOvrkyVWKiYiISHN02nIzZcoUhIWFoWnTpsjJycHatWsRHR2NXbt2AQBGjBiBxo0bIzIyEgAwceJE9OzZE/Pnz8eAAQOwbt06xMbGYtmyZbqsRrU0kD5+q4sVSpgY611eSUREZBB0mtykp6djxIgRSElJgY2NDQICArBr1y707t0bAJCUlASjJ7Yr6NKlC9auXYsvvvgCn3/+Oby8vLBp0yb4+fnpqgrVZmFqLP6cX6SAjMkNERGRRujdOjeapqt1bq6m5aD3fw8CAI5/3gvOMnOtvTYREVFdVyfXuTF0Xs7WMDMpebvz5Bx/Q0REpClMbrSosLhkIb+7mQU6joSIiMhwMbnRgU83nNN1CERERAaLyY0O3M1iyw0REZGmMLkhIiIig8LkhoiIiAwKkxstau9up+sQiIiIDF6tkpvk5GTcvn1bfH7ixAlMmjSpTqwUrEuD2zUBAPT2da6iJBEREdVWrZKbYcOGYf/+/QCA1NRU9O7dGydOnMDUqVMxa9YstQZoSEoXJVYo69W6iURERFpVq+QmLi4OHTt2BAD8+eef8PPzw9GjR/Hbb79h1apV6ozPoBj/u5XE9Xu5Oo6EiIjIcNUquSkqKoJUKgUA7NmzBy+99BIAwMfHBykpKeqLzsDE380CACTez9dxJERERIarVslN69atsWTJEhw6dAhRUVHo168fAODu3btwcHBQa4CG5MLtLF2HQEREZPBqldx88803WLp0KUJCQhAREYHAwEAAwJYtW8TuKiqLI22IiIg0z6Q2F4WEhCAjIwPZ2dmws3s8vfmdd96BpaWl2oIzNHaWZuLP93PlcLCS6jAaIiIiw1SrlptHjx5BLpeLiU1iYiK+//57JCQkwMnJSa0BGpL3n/cUf/71WJIOIyEiIjJctUpuBg4ciF9++QUAkJmZiU6dOmH+/PkIDw/H4sWL1RqgIWkgfdxQtu4kkxsiIiJNqFVyc/r0aXTv3h0AsGHDBjg7OyMxMRG//PILfvjhB7UGaFgej7qRmZvqMA4iIiLDVavkJj8/H9bW1gCA3bt3Y/DgwTAyMkLnzp2RmJio1gANSYuGVuLPAwJcdRgJERGR4apVcuPp6YlNmzYhOTkZu3btQp8+fQAA6enpkMlkag3QkEgkErzR2R0AUKxQ6jgaIiIiw1Sr5GbatGn4+OOP4eHhgY4dOyI4OBhASStO27Zt1RqgoSn+d+uFH/Zd03EkREREhqlWU8FfeeUVdOvWDSkpKeIaNwDQq1cvDBo0SG3BGaIDCem6DoGIiMig1Sq5AQAXFxe4uLiIu4M3adKEC/hVg0QiEX9WKgUYGUkqKU1EREQ1VatuKaVSiVmzZsHGxgbu7u5wd3eHra0tZs+eDaWSY0kqMynUS/x5wd6rOoyEiIjIMNWq5Wbq1Kn43//+hzlz5qBr164AgMOHD2PGjBkoKCjAV199pdYgDckrQU3wyYbzAIBjN+7rOBoiIiLDU6vkZvXq1Vi+fLm4GzgABAQEoHHjxhg3bhyTm0o82S11/OYDHUZCRERkmGrVLfXgwQP4+PiUOe7j44MHD/iFXRULU2Ndh0BERGSwapXcBAYGYtGiRWWOL1q0CAEBAc8clKGTFyvEn3MKinQYCRERkeGpVbfU3LlzMWDAAOzZs0dc4yYmJgbJycnYvn27WgM0dFfTc9GuqV3VBYmIiKhaatVy07NnT1y5cgWDBg1CZmYmMjMzMXjwYMTHx2PNmjXqjtHgfDHAV/x58E9HIQhCJaWJiIioJiSCGr9Zz507h3bt2kGhUFRdWEeys7NhY2ODrKwsnW0VIQgCmk153MI1tmcLTA4rO4aJiIiIStTk+7tWLTf0bJ6cMQUASw5c11EkREREhofJDRERERkUJjdERERkUGo0W2rw4MGVns/MzHyWWOq15Ydu4O3uzXUdBhERUZ1Xo+TGxsamyvMjRox4poDqi83ju2Lgj0fE51/+cwktna3Ro2VDHUZFRERU96l1tlRdoA+zpUp5fr4dxUrVtz9uZl9YSWu9WTsREZFB4mypOuL9573KHPObvovr3hARET0DJjc6NP65FuUeLyhSajkSIiIiw8HkRodMjMt/+4Pn7MX+hHQtR0NERGQYmNzoocz8IoxeeRKtp+3E7Yf5ug6HiIioTmFyo2PTX/St8FxeoQKztl7UYjRERER1H5MbHRvdtRm+GuRX4fndF9O0GA0REVHdx+RGD7za3q3S819vv4Tv91wBAMTdycKRaxkoVnDQMRERUXm4oIoeMDU2wrnpfRA4c3e555cdvAEAeLldE7yw8LB4fPf/9UBLZ2utxEhERFRX6LTlJjIyEh06dIC1tTWcnJwQHh6OhISESq9ZtWoVJBKJysPc3FxLEWuOjYUpbs0ZUGmZ7nP3qzzneBwiIqKydJrcHDhwAOPHj8exY8cQFRWFoqIi9OnTB3l5eZVeJ5PJkJKSIj4SExO1FLHm2VmaVrvs4WsZUCi54B8REdGTdNottXPnTpXnq1atgpOTE06dOoUePXpUeJ1EIoGLi4umw9OJX97shBcXHa664L9afL4dLwY2wowXfeFgJdVgZERERHWDXg0ozsrKAgDY29tXWi43Nxfu7u5wc3PDwIEDER8fX2FZuVyO7OxslYc+829ig+Uj2qOJnUW1r9l67i6CvtyD/MJiAMB/o65g0b6rmgqRiIhIr+lNcqNUKjFp0iR07doVfn4VT4329vbGihUrsHnzZvz6669QKpXo0qULbt++XW75yMhI2NjYiA83t8pnJumDUF9nHP7seYR412yH8C82xWHPxTQs2HsV3+6+gmvpuRqKkIiISH/pza7g7733Hnbs2IHDhw+jSZMm1b6uqKgIrVq1QkREBGbPnl3mvFwuh1wuF59nZ2fDzc1NL3YFr8qei2l4+5fYWl/vIjPHsc97qTEiIiIi3ajJruB6MRV8woQJ2LZtGw4ePFijxAYATE1N0bZtW1y7dq3c81KpFFJp3RyLEurrjL0f9YSbnSVOJz1E3J0sfPnPpWpfn5pdoMHoiIiI9JNOu6UEQcCECROwceNG7Nu3D82aNavxPRQKBS5cuABXV1cNRKh7LRpawczECJ2bO+Dt7s1xa84AhLZyrvb1pxIfIqegCDO2xOO9X09BTxrqiIiINEanLTfjx4/H2rVrsXnzZlhbWyM1NRUAYGNjAwuLkgG1I0aMQOPGjREZGQkAmDVrFjp37gxPT09kZmZi3rx5SExMxNtvv62zemjb8pHt8ahQgVbTdlZZ9uXFR+HjYo3LqTkAgIsp2WjdyEbTIRIREemMTpObxYsXAwBCQkJUjq9cuRKjRo0CACQlJcHI6HED08OHDzFmzBikpqbCzs4OQUFBOHr0KHx9K96A0hBZmBmLPztaSZGRK6+wbGliAwC3Hz5ickNERAZNbwYUa0tNBiTpO4/J/wAAOjazx7QXfFW2ZqjMgABX/DisnSZDIyIiUquafH/rzVRwqrn5QwLh6WSFb14OgF9jG9yaMwCf9vOu8rp/zqdAyZWNiYjIQLHlxgDF3cnCzrhUnE3OxOFrGeWWsZaa4MLMvlqOjIiIqHbq3FRwUi+/xjbwa1wyrmbZwev4evvlMmVy5MXaDouIiEgr2C1l4CSQVHhu76U0LUZCRESkHUxuDJyk4twGb62O5bo3RERkcJjcGLghQW5wspYi0M223PP/jbqi3YCIiIg0jMmNgbOxNEXMlF74893OsDYvO8Tqh33XkMvxN0REZECY3NQDxkYSSE2McXJqKK5+FQYLU2OV837Td+HPk8k4m5ypmwCJiIjUiMlNPWJuagxTYyMc/uy5Muc+/es8wn88ooOoiIiI1IvJTT3kYCWFraVpuec4wJiIiOo6Jjf11LEpvco9vuXcXS1HQkREpF5Mbuop86fG3ZSauO4sZm29iGKFUssRERERqQeTm3pscNvG5R5fceQm/jp9W8vREBERqQeTm3ps7isB6O/vUu65z/66gCl/X+AGm0REVOcwuanHTIyNsDCiXYXnfz+RhANX72kxIiIiomfH5KaeMzaSYMJznhWe/2HvVS1GQ0RE9OyY3BA+7uuNhC/7lXvuTFImPttwnlPEiYiozmByQwAAqYlxhdPD/4hNxvYLqVqOiIiIqHaY3JBIQMWtM6tjbmHNsUTkF3IfKiIi0m9MbkjkIjNHV0+Hcs+duPkA/9kUh1lbL2o5KiIiopphckMiiUSC397uXGmZdSeToeD0cCIi0mNMbqiMoe3dKj3f4vPtuJP5CN/vuYKDVzhVnIiI9ItEqGfTYLKzs2FjY4OsrCzIZDJdh6OXBEHA7YePkF1QhAE/HK6y/K05A7QQFRER1Wc1+f5myw2VIZFI4GZvidaNbPDtkEBdh0NERFQjTG6oUq8ENamyTOytB1qIhIiIqHqY3FCVVo7uUOn5V5bEIOl+vpaiISIiqhyTG6qSq415lWXm7LwEpVJArpzr4BARkW4xuaEquds3qLLMw7wijFx5An7TdyH5AVtxiIhId5jcUJUszIyrLBNz4z4OXc0AAHSfu1/TIREREVWIyQ3VyOf9fXQdAhERUaWY3FC1dPCwAwAMCGik40iIiIgqZ6LrAKhuWPdOMPILi2Ftblqt8rG3HqC9h72GoyIiIiqLLTdULcZGkjKJTZifS4XlX1kSA4/J/6CeLYBNRER6gMkN1dh3rwYizM8FI7t4VFk2kevfEBGRljG5oRob3K4JFr8ehPbudvBxsa60bMi30Wy9ISIirWJyQ7VmYmyEHRO744eItpWWW3bwhpYiIiIi4q7gug7HYBQrlMgpKEbb2VHlnu/SwgGrRneEmQnzaSIiqjnuCk5aZ2JsBLsGZrj6VVi5549ev4+WX+zA4ujrWo6MiIjqGyY3pFamxpX/Sn2z87KWIiEiovqKyQ1p3fJDHINDRESaw0X8SOu+/OcSrqTlwMXGAh/2bqnrcIiIyMAwuSG12zA2GLcfPkJ3L0cEfbmn3DJ/xt4GALzWwQ2uNua4k/kIjW0tIJFItBkqEREZIHZLkdq197BHeNvGcLCS4vcxnSstmycvxtKDN9Dtm/2Yv/uKliIkIiJDptPkJjIyEh06dIC1tTWcnJwQHh6OhISEKq9bv349fHx8YG5uDn9/f2zfvl0L0VJtBLdwQMdmFe8x1fu/BzFnR8kg40X7r2krLCIiMmA6TW4OHDiA8ePH49ixY4iKikJRURH69OmDvLy8Cq85evQoIiIi8NZbb+HMmTMIDw9HeHg44uLitBg51cS0F3xrVL6eLb1ERERqpleL+N27dw9OTk44cOAAevToUW6ZoUOHIi8vD9u2bROPde7cGW3atMGSJUuqfA0u4qcbyQ/y0X3u/irL9fZ1xpmkTKwd0wktnSvf2oGIiOqPOruIX1ZWFgDA3r7iboyYmBiEhoaqHOvbty9iYmLKLS+Xy5Gdna3yIO2r7jjhqItpyMiVY/a2i5oNiIiIDJbeJDdKpRKTJk1C165d4efnV2G51NRUODs7qxxzdnZGampqueUjIyNhY2MjPtzc3NQaN1WPs8wcVlIT2FqaYnKYT5XlD13NwPzdVY+/IiIiepreJDfjx49HXFwc1q1bp9b7TpkyBVlZWeIjOTlZrfen6jE1NkLsF6E4NqUXlNXsCV247xpuP8znGBwiIqoRvUhuJkyYgG3btmH//v1o0qRJpWVdXFyQlpamciwtLQ0uLi7llpdKpZDJZCoP0g1zU2OYmxrjxYBG1b7mu91X0OGrvbialoOdcSnIKSjSYIRERGQIdDqgWBAEvP/++9i4cSOio6Ph5eVV5TVDhw5Ffn4+tm7dKh7r0qULAgICOKC4Dsl6VAQrqQlmbY3H6pjEGl17M7I/F/sjIqpn6syA4vHjx+PXX3/F2rVrYW1tjdTUVKSmpuLRo0dimREjRmDKlCni84kTJ2Lnzp2YP38+Ll++jBkzZiA2NhYTJkzQRRWolmwsTGFsJMHMgX5oYGZco2tjbtzXUFRERGQIdJrcLF68GFlZWQgJCYGrq6v4+OOPP8QySUlJSElJEZ936dIFa9euxbJlyxAYGIgNGzZg06ZNlQ5CJv0W0bFpjcoP+/k4x+EQEVGF9GqdG21gt5T+eVSowMqjN3E5JQdbzt2t1jWHP3sORhIJXll8FG72llj9ZkeYm9asBYiIiOqOmnx/c+NM0jkLM2OMC/EEgGonN8OXH4e7QwPczSrA3awCLDlwHW92awYAkJmbaixWIiLSf3oxW4qo1E/D21WrXOL9fBy8ck98fjY5EwEzdiNgxm4UKZSaCo+IiOoAJjekV/r7u+JmZP8aX3cvRy7+nFtQrM6QiIiojmFyQ3qnNtO84+8+3lajkC03RET1GpMb0ktbJnSt9bWdvt6LxPuqO8sfv3EfQ5fGICE151lDIyIiPcfkhvRSQBNbrH6zo/h8bM8WNbp+6sY4nEvOFJ8PXXYMx28+wNu/nFRXiEREpKc4W4r0Vg8vR0zt3wqtXGXo5uWIJQeuV/vaw9cycPhaBvr7u6C9++Nd5tOy5ZVcRUREhoAtN6S3JBIJxvRojm5ejgCAea8E1Pge2y+kYta2i4/vqbboiIhIXzG5oTpjSHs37JrU45nuwS2piIgMH5MbqlO8Xazh7WwNo1omKQVFSmTlP95ZPFdejGLOriIiMihMbqjO2fV/PXAjcgBsLWu3EnHgrN14mFeIU4kP4Td9Fwb9dBSHrt7D7G0XkZHLMTlERHUd95aiOutyajb6fX9IrfeUmZvg/Iy+ar0nERE9u5p8f7PlhuosHxcZtn/QXa33zObqxkREdR6nglOd5ttIhpuR/SGRSPDP+RSMX3v6me+ZmlUAFxtzNURHRES6wJYbqvNKt2vo7++CFwJcn/l+nSP3AgCUSgHf7krA0hqsr0NERLrHMTdkkPLkxbA0M8aLiw4j7k521RdUIbSVEzp42OPdclZKVipL/gkZ1XYKFxERVYljbqjeayA1gUQiwfp3u2DBa21gZvxsv+p7LqUjcsdl3H6YD3mxAop/ExqlUsALCw/jxUWHxSSHiIh0i2NuyKBZmBljYJvGGODvCs+pO575ft2+2Q8TIwmaN2yA3f/XExm5clxMKWkZynpUBLsGZs/8GkRE9GzYckP1gomxEQ5+8pxa7lWsFHAlLRcbTt1WOc52GyIi/cDkhuqNpg6WWPpGkPjc/hlbWT5efw4dv94rPi8dvnYrIw+Hrt57pnsTEVHtcUAx1TuCIKBYKcDU2Agek/9R231HBLtj5kut0WzKdgDApvFdceLmfeyOT8OqNzvCSspeYCKi2qrJ9zf/b0v1jkQigalxycym1o1kiL/77LOpAOCXmETcefhIfH7+dia+3n4ZALDuRBLe7t5cLa9DRESVY7cU1Ws/DmuHgW0aYdP4rmhqb/nM99t7OV38WV70eEPOL/+5BHmxQnx+KSUbf8YmY92JpGd+TSIiUsVuKaJ/3cl8hKUHruOXmESN3H/8cy3w4/7r6NLCAUev3xePvxfSAj28GiK4hYNGXpeIyBDU5PubyQ1ROR7mFeLr7Zew/qkZUZp0a84Arb0WEVFdw0X8iJ6RXQMzzBsSqPXXFQQBufLyN++8nJqN5Af5Wo6IiKjuYXJDVIlmjg209loKpYCP/jwHv+m7cPGpQc4ZuXL0+/4Qus/dr7V4iIjqKiY3RJVY+kYQuns54u9xXbBpfFc8591QY6/V4vPt+PvMHQBA/x8O4bfjich6VAQASLyv2mKjVAq4nyvXWCxERHUZx9wQ1YC8WIFTiQ/R3t0e7b+MQnZB+V1I6tKlhQPWjumMU4kP8fLiowCAL8P9sPnsHZy89RBrx3RClxaOGo2BiEgfcMwNkYZITYzRpYUjzEyMsO397hp/vdJZVQv3XRWPfbEpDidvPQQALDlwA9fSc7Fo31XkF2o20SIiqiu4iB9RLTV1ePZ1capjzbFERCeUv53DwSv3EPrdAQBAeo4cswb64VGhAquO3kK7prbo1JzTy4mo/mHLDZGaqGMRwPL8Z1NctcqVJkDf77mCb3ZextBlx5CQmqORmIiI9BmTG6Jn8M8H3QAAjW0tsH1id9yaM6DS9Woa21poLJakB/k4eeuBygKBM7bEI6egCEUKJcauOYXwH4+goEhR4T1y5cXYn5CO2dsu4pudlzUWKxGRJnFAMZEGlG7IGd6mEe7lynHkWknC8UZnd2y/kIL7eYU6i21IUBMEutlCAPB6p6aQSCTiuaFLY3D85gPx+YUZfWBtbqqDKImIVHHjTCIdW/t2J2w9n4KpA1rBSmqCV5fE4MStBxjawQ2zw/0wdeMF/HZcN/tKrT91W1x52aGBGfr7u+Jccia+2n4JJ55IbABAqSzvDiUUSgFFCiXMTY01GS4RUY2x5YZIC4oVSjzIL4STtTmAksRg2/m7mLjurE7jau9uh9jEhxWePzG1FxLv56ONmy1MjVV7sQf/dATnbmfh9Be9cSU9By2drGFjyVYeItIMTgUn0jMmxkZiYgMAxkYSDGzTGGN7ttBhVKg0sQGATzecx5AlMZizo+z4m9NJmVAoBUzZWFJm4I+HNRUmEVGNMLkh0qHP+nkjyN0O7Zralnv+ieEwOlE6A+t/h2+qHI+6mCb+vP1CKgDg1v18KJSCOGB53q7LeH35cdzPlWP+7gRcS8/VUtREVN+xW4pIx0r/CTabsl081tLZCiOCPTC8U1OV47r0To/mMJJIsOTA9QrL+LhY43JqDt7u1gzL/02IGpgZI69QAamJERK+DAMApGQ9wt+n7yCiY1PYNzDTSvxEVLfV5PubyQ2RnrialoPNZ+/inZ7NIXtihtK0zXH4JSYRAGBtboKcgmI0tbdE1qMice+puqJ0mnyf/x7AlbRcdPdyxOLXg7AmJhH9/V3g7qC9jUqJqG7hbCmiOsjL2Rof9/Uuc9zF5vFYnaOTn4epsRHMTY1RUKRAx6/2aHx/K024klbSRXXoagZmb72IP2KT8c3Oy+jgYYdZA/3QypV/eBBR7XHMDZGeG97JHc0cG+Ddns1hbW4qTr02NzXGqf/0RtzMvvjlzY4I8W6II5OfR9zMvjqOuGLLD90oc+yP2GTx55O3HiJswSE8yCvEpxvO4ei1jHIXHcx6VASFsl41OhNRDbBbisgADfv5mMpKxfrkpcBG2HLubo2umfdKAIa0dwMAJN7PQ8950ejUzB4dPOzxIL8QiffzIDM3xeLXgzQRMhHpAU4FJ6rnvn+tDd4LeTzN3L+xDQBgxaj2mPtygK7CAoAaJzYA8MmG8yhSlKwo+PfpOwCA4zcfYNH+a1h7PAlHrt3HjrhUXLidpdZYiahu0mnLzcGDBzFv3jycOnUKKSkp2LhxI8LDwyssHx0djeeee67M8ZSUFLi4uFTrNdlyQ/XJvRw5sh4VwtPJusy5NccSq70ppz6wNDNG1Ic90XXOvkrLRXRsik1n7qBPa2cMatsY7T3scfhqBrp7OaKBtGSYYUGRAssP3UCItxP8/k38iEi/1ZmWm7y8PAQGBuLHH3+s0XUJCQlISUkRH05OThqKkKhua2gtLTexAUr2ubrxdX9MCfPRclS1k1+oqDKxAYDfTyThUZECm8/exaiVJ+E3fRfG/noKk/44K5ZZceQmvt19BS8sLLvw4K74VHz05zk8KlTgu90J2B2fKp4rVihRrCh/TwpBEDBr60WsPnqrxnUjIvXS6WypsLAwhIWF1fg6Jycn2Nraqj8gonrGyEii84UCteXJhQcTUnNUzqVlF2BnXCoGt2uMd9ecAgDcyMjFmaRMAIDUxAgtna1x4U5Jt9f1r/vD2Ej1jTudlIkVR0rW9hnZxUNDtSCi6qiTU8HbtGkDuVwOPz8/zJgxA127dq2wrFwuh1wuF59nZ2drI0SiOu3FwEbIlxdj7+V0vBDgitaNbKAUBMzblaDr0J7J19sv4WpaDvb/u/IyoNo9N31LvHi8NLEBAHmxUkxsgJLuvsJiJdzsLcRd1bMLHq85JAgCVh29BR8XGYJbOGiqOkRUgTqV3Li6umLJkiVo37495HI5li9fjpCQEBw/fhzt2rUr95rIyEjMnDlTy5ES1R1N7S3Fn1eO6oDYxAf4qLc3jIwkkBcrYGZsBIlEAuGp5GZM92b4+dDN8m6pt5YdLDsVvTbjjjpH7gUAfNi7JVo3ksG3kQx4YvTi4WsZmLn1IoDHCxc+i2KFEibGnP9BVF16MxVcIpFUOaC4PD179kTTpk2xZs2acs+X13Lj5ubGAcVE/xIEAT/uv4ZAN1t092pYaVmPyf8AAML8XPDT8HZ6szWEPrs1ZwAEQUBBkRIWZsbVvq70mplb47H13F3s/zgETjLzqi8kMlB1ZkCxOnTs2BHXrl2r8LxUKoVMJlN5ENFjEokEE573qjKxAYC3ujVDaCtn/DisHSQSCb57NRAh3hVf193LETcj+2PnpO7qDLlOSc0qQHDkPrSatlMcnJxTUITAmbsxfXMcfjueiNsP88tc95/NcWg1bSfWnUxGXqECLy06ghlb4vHk36NZj4rw4sLDWHbw8X5fWflFyC+se6tWE6lTnW+56d27N6ytrfH3339XqzynghOpX2mLTqluno54q1szdGhmD6t/p18XK5T4YN0ZcRfxfR/1xC8xiVhVz2YX3ZozAJ+sP4f1p26Lx6ylJgjzd0F+oQKLhpV0sT/9npZaNboDQrxLZoh+tzsBP+wr+ePuZmR/5Bcq0Hr6LkgkwM3Ix91hWY+KkHQ/H/5Nyp/2XqRQYurGCwhu4YBBbZuopZ5E6lZn9pbKzc1VaXW5efMmzp49C3t7ezRt2hRTpkzBnTt38MsvvwAAvv/+ezRr1gytW7dGQUEBli9fjn379mH37t26qgIRAbCxMFXZxPP1zk3xnI/qEg0mxkaY+0ogQlo6obevM+wamCG8bWMxuXG0kmLvhz0ROMuw/z2Xl7TkyIvxZ2xJsrPt/D8IbVXx8hYP8grFnx89sTXF0KXHMO1FXwCAIAC741ORniPH653dMeCHQ7j98BHWvt0JXTwdy9xz4+k7+DP2Nv6Mvc3khgyCTpOb2NhYlUX5PvzwQwDAyJEjsWrVKqSkpCApKUk8X1hYiI8++gh37tyBpaUlAgICsGfPnnIX9iMi7dnzYU/E381Cp2YOSHyQB2/n8tfWsZKa4NUObirPSx369DlYmBljyevtsP/yPZU9p570SV9veDg0wPi1p9VbCT2y51J6hed+ir6O4BYOeLrN/cStByrP3/l3SvsXTwyYHrb8OI5Ofh6NbC1UymbkyfGslEoBqdkFZe5NpAt60y2lLeyWItIv30VdQUMrM7wR7KFy/MkWjhkv+iKiU1Nk5hfB+d9BtRV12wBA39bO2BWfVuF5Q/V2t2ZYfrjyGWwtna2wcnRHfPjHWXRqZo//690SP0VfF2fCbRrfFT4u1uIGrdX16YZz+DP2Nr4f2gbhbRvjYV4hpm+Jx6vt3dDNq2xrEVFN1asBxURUt33Yu2WZxAYAzE1L/ve04LU2GNW1GaQmxmJiA5QsrFeqdSMZvJysAAB7PuyBJa8H4dz0PuI9Krq3oakqsQGAK2m5eON/x3H85gP8sO8adsalqpwP//EIxvwSKw5cVigFCIKAIoUSPx+8gYt3y18rrLRb7buoKwCAyB2XsOXcXbz+v+NVxhSdkI5bGXni8+QH+WX2CVMqBczYEo+//h2rdC09F68ti8GqIzcx7rdTuJqWg1OJD/D26lgk3s8D1W9suSEivZQnL8adzEdoWUEXV05BETLziyAvVqCJnSXMTY2hVAowemLl4Au3s/Dy4qMofGLLhKHt3bAvIR33cp69K8aQdfdyxM8j2qPv9wfR0tkanZrZ48t/LgEA4mf2FffpKlXaktbEzgKHP3tepWXt6bV+rqXn4H5uITo1d8CpxAd4eXGMSrnSaw9/9hya2FkiT16Mo9fvY8wvsWK5ft8fxOUnVpp2tDJDRm7JeCS/xjJse7/+ztAzVHVmQDERUUUaSE0qTGwAwNrcFNbmpirHjJ7aEsG/iQ2ufBWGP2OTYW9phrzCYvTxdYFys6AyW6kqDa2l9S4ZOnQ1A4N+OorE+/lIvJ+vsn3Fq0tj4OHQAP9cSEHsF6FwtJKK524/fIQFe66q3GvDqdvILyyGQinA2twUH68/V/Ianz6H85Xs5H45JQfZj4rR/4dDZc6lZReoPC9NbAAg+cGjKusXfzcLE9edxSd9vdG3dfU2Xqa6g8kNERm8V9u7qTyf/lJrleRmyetB2BGXgtSsAgzv7I6v/rmItOySZKaxrQX2ftQTu+JTMXHdWW2GrXOXUsrvgoq/m434f7un+i84hHXvdFY5/989V1SelyYzT0t+kK8yMDpy+yV81u/xRq5Zj4qw+dzdMtcdvZ6B6nQ55MmLYWpsBDOTst2QE9aewc2MPLy75pRaVpEm/cJuKSKql2ZsiRenoZf35dZz3n4k3s/HoU+fg9u/W1RcSslG2IKyrQil/vmgG5o7WqHVtJ0aibk+GtS2MTaeuVOja2wsTHF08vNoPX0XgJIxWROe80RTB0vM3ZmASaFeeHPVSTzML1m+4PLsfjA3NUauvBgNzIzF/cL0wd3MR9h+IQWvdnCD7KmWyvqmJt/fTG6IqF56VKjAprN38LyPk8pA5VLyYgWyHxWjofXjLpcLt7Pw4qLDAEpadO5kPoKzTIrdk3rCxvLxF8/CvVcxP+pKmXtW5eKsvth3OR0T1p6pRY2olLW5CT7t51PpvmEycxNkFzxeydlIAigFoKunA5o7WsHL2QpvdHYX91X7/UQyziQ9RESnpmhiZwEn67K/MwVFCqw9noQQ74Zo3tBKLXXpErkXd7MKMLhtY3w3tI1a7llXMbmpBJMbIqotQRDw/u9n4OHQAC8HNcFP+69hbEgLtHjqi0ypFND885J9t357uxNyCoowY8tFvBHsLk65drO3UBkb8sHznviwjzeAkoX62s2OEs95OlnhWnqupqtHT5n7cgAGBLiKLUBPKm3ti7l+HzviUjA5zAdLoq+LK0Y/3RooCEK5LULnb2ci61FRhduflA6ubmgtxcmpoc9Un7qOA4qJiDRAIpGI2yMAwLwhgeWWMzKS4MqXYUjLLhC7tPr5uaKwWIm/Tt+Gm50lJof5iF1cswe2RkTHpuL19g3M0NyxAW78Oz26c3P7Wic3XwxoJc5yoppZdzIJvxy7Ve655YduYNXRW7j9sCRBtZKa4FTiQ/H8vRy52Oq3OPo6lh+6gXXvdMa9HDnaNrUTN1F9adERAMAHvbzQs6Ujmjtawa6BGQBg5ZHHU/tNjMomRvdy5CgoUoi/Y/QYW26IiLRIoRRgJClJlKIT0tHEzgKeTmVnhcmLFfD+omTszke9W6Knd0Pxi7BfaxfsjH+8Pk2gmy3OJWeWuUd3L0eseauTyrTsJ5Mm0qylbwThs7/OIzO/SOW4j4s1pvRvhYNX7uF/5axNdGvOACQ/yEf3uftVjkd/HAIPxwbi89LP9cx/eosJUXUolQIup+bA28UaxuUkTfqKi/gREekpYyOJ2D0R4u1UbmIDAFITY7wS1ARWUhO81rEpAprY4vjnvbBqdAcsfr0dLs3qJ5adFOqFy7P74cnvqed9nLAool2Z++77OAR/j+uC/v6c/qxp7645VSaxAYDLqTkYueJEuYlNqbuZZaezh3wbjbdXn8S19ByV49fulbTqCYKAj/48h//+O94rp6AIf8YmIzO/UKX8nJ2X0f+HQ4jcbrgtekxuiIj01LdDAnFmWm+xe8NZZo4QbydIJBKxWwMo6RIxNzXGmO7NxWMrRnUQBzm/FNhI5b7tmtrh60H+4nMnayksKthu4c93g9VWH6qeievOYNH+a+We23MpHaHfHcSSA9fFYwplSQfMqcSH+Ov0bSzYexWCIGDy3xfw6YbzeO/Xx/uwpecUYNnBGwDKrmhtSB05TG6IiPSYqXHF/5v+tJ83hgQ1QXt3OwDAiC4eAFBmV/Enk55StpZm+DLcD18P8seJqaG4NLtfmTJN7CzQ3t2uwsRn3isB1a1GpV58Kvmq7zafvYtDVzMqLTNnx2Xx59eWHcP+hHS8siRGPPb9nqv453wKACDmxn3cfpiPzzdeQMev9pZ7v9VHb6Hd7Cj8EnMLkTsuQV6swOLo61hx+CYEQUDS/XwIgoDCYiUu3s3W+0SIY26IiAxIrrwY5iZGMHkiKRIEATO3XkQTOwu8XU6iU+rJsTlvdWuGEcHucHcoGeMxZ8dlsbXAWSbF0jfao42bLVpO3aGyvUWpbe93g7PMHB2+2qNyvF9rFxQrlTh+8wF6+zpj1kA/WElN8Pfp2/jwz3MY1LYxvh0SiBb/zjYjzeru5VhuIhXRsSl+P5EE4PGyB5/188HppIeIupiGWQNb4+LdbNy6n4cfXmuLBlITZD4qwswt8Xi7e3N0bGav9lg5FbwSTG6IiMpX2X5QMdfvI+LnY2XOKZQCjt24Dy8nKxy8moETN+9jxkutYWlmonLP5o4NsH5sMBz+3arh6X3AgJIVixvbWsDISIK4O1lYeeQWvhrkB5//cFHEuqa8/ceeFaeCExFRjRkbSaBQCuXOoAlu4YDvXg2E11MDoI2NJOjq6QgAeCWoCV4JalLuvc1MjMTEBii7DxgAlSnNfo1tMP/VslPtD336HIyMJDhyNQOf/nX+qestyt1X6ukF+0jzWk/fpdNtLTjmhoiIAAAbxgajXVNbrB9b/iDiwe2awL+JTY3uaf3vX+/dvRxrHZfMvOQePw5rBzd7SzS2tcCrHdwQ9X894GrzeKXgX9/qhN/HdC5z/bb3u5c7bmjeKwHl7jtFdR+7pYiISGOSH+Rj3+V0DO3gBvMKBiZX5X6uHJdTc9ClhUO5q/w+vfqvvFiBcb+ext7L6fB1lWH7xO7iuae73nIKinDwSgbGrz2tcs/RXT2w8sitWsVLJdTdcsMxN5VgckNEZPgeFSqwMz4FPVs6wf6JBe5uP8zH3J0JmD3QT5wqn3g/Dz3nRYtl/nw3GB2b2SM9uwCL9l/D+tjbeL+XJ5o5NMB7v5UkQT4u1ricmoPQVk54q1tzJKRmY8bWiyoxlO5XVV8xudEiJjdERPSk2w/z0e2bx6sBV/alXKxQ4mF+kcqGqqXGrz0tTr+eHOaDd7o3F/cYq8yC19pg2uZ4LB/ZHkUKJYb9fLzS8tZSE+TI9X8MkS6TGw4oJiKieq2JnWW1Bx2bGBuVm9gAwPwhgXijszvau9uJU/FfCHDFtn8THgAwNZagSPG4TaG5YwMMbNMYLwU2ErvWbs0ZgKiLaXhnTSxeCmwEZ5m5uPDe3FcC8FJgI7y8+Cji72bXus6Gji03RERU751KfIiXFx8FoN4WhyKFEtfv5aKlkzUKihVIepCPj9efQ9ydksSko4c9/qxgAHdhsRJmJkYoLFai5Rc7AABXvgxTGQStVAoQACw5cF3ccb7U9a/7q6wXtHJ0B4xeeVJtdauKLltuOEyciIjqvSB3O8x40Rer3+yo1vuaGhvBx0UGIyMJLM1M4OMiw7b3u2PV6A4Ibu6AbyvYWR6AmMSYmRgh9otQxH4RWmZ2l5GRBMZGEox/zhOHPn0O56b3Ec89PaXf6akWpx8i2la6caaZiRE2juuicszN3qLyCv+rjZtttcppCruliIiIAIzq2kxrrxXi7YQQb6eqC/7L0ar8rrAnla4TdPzzXpD+mwS926M5lh68gdaNZPB1lWFksDvMTY3R07shgps7YO3xRBy78QANraUY4O+KVUdvife7NKsfjI0kcJGZIzW7AACwMKIdwn88IpZZ+kYQDl65hx4tG2LGlnikZJWU+2JAq2rXTRPYLUVERGSgihVKnE3OREAT23LX9EnPKcCKw7cwrGNTOMmk2BmXipSsAjRztEQ/P1cAJbuLz999BS8GuiLI3R7f7U7A1vMpWDumE1xtVFtyrqTl4GZGHvq2Vv+u85wtVQkmN0RERHUPx9wQERFRvcXkhoiIiAwKkxsiIiIyKExuiIiIyKAwuSEiIiKDwuSGiIiIDAqTGyIiIjIoTG6IiIjIoDC5ISIiIoPC5IaIiIgMCpMbIiIiMihMboiIiMigMLkhIiIig8LkhoiIiAyKia4D0DZBEACUbJ1OREREdUPp93bp93hl6l1yk5OTAwBwc3PTcSRERERUUzk5ObCxsam0jESoTgpkQJRKJe7evQtra2tIJBK13js7Oxtubm5ITk6GTCZT6731gaHXDzD8OrJ+dZ+h15H1q/s0VUdBEJCTk4NGjRrByKjyUTX1ruXGyMgITZo00ehryGQyg/2lBQy/foDh15H1q/sMvY6sX92niTpW1WJTigOKiYiIyKAwuSEiIiKDwuRGjaRSKaZPnw6pVKrrUDTC0OsHGH4dWb+6z9DryPrVffpQx3o3oJiIiIgMG1tuiIiIyKAwuSEiIiKDwuSGiIiIDAqTGyIiIjIoTG7U5Mcff4SHhwfMzc3RqVMnnDhxQtchVcuMGTMgkUhUHj4+PuL5goICjB8/Hg4ODrCyssLLL7+MtLQ0lXskJSVhwIABsLS0hJOTEz755BMUFxdruyqigwcP4sUXX0SjRo0gkUiwadMmlfOCIGDatGlwdXWFhYUFQkNDcfXqVZUyDx48wPDhwyGTyWBra4u33noLubm5KmXOnz+P7t27w9zcHG5ubpg7d66mqwag6vqNGjWqzGfar18/lTL6XL/IyEh06NAB1tbWcHJyQnh4OBISElTKqOv3Mjo6Gu3atYNUKoWnpydWrVql6epVq34hISFlPsOxY8eqlNHX+gHA4sWLERAQIC7iFhwcjB07dojn6/LnB1Rdv7r++T1tzpw5kEgkmDRpknhM7z9DgZ7ZunXrBDMzM2HFihVCfHy8MGbMGMHW1lZIS0vTdWhVmj59utC6dWshJSVFfNy7d088P3bsWMHNzU3Yu3evEBsbK3Tu3Fno0qWLeL64uFjw8/MTQkNDhTNnzgjbt28XHB0dhSlTpuiiOoIgCML27duFqVOnCn///bcAQNi4caPK+Tlz5gg2NjbCpk2bhHPnzgkvvfSS0KxZM+HRo0dimX79+gmBgYHCsWPHhEOHDgmenp5CRESEeD4rK0twdnYWhg8fLsTFxQm///67YGFhISxdulTn9Rs5cqTQr18/lc/0wYMHKmX0uX59+/YVVq5cKcTFxQlnz54V+vfvLzRt2lTIzc0Vy6jj9/LGjRuCpaWl8OGHHwoXL14UFi5cKBgbGws7d+7Uef169uwpjBkzRuUzzMrKqhP1EwRB2LJli/DPP/8IV65cERISEoTPP/9cMDU1FeLi4gRBqNufX3XqV9c/vyedOHFC8PDwEAICAoSJEyeKx/X9M2RyowYdO3YUxo8fLz5XKBRCo0aNhMjISB1GVT3Tp08XAgMDyz2XmZkpmJqaCuvXrxePXbp0SQAgxMTECIJQ8kVrZGQkpKamimUWL14syGQyQS6XazT26nj6y1+pVAouLi7CvHnzxGOZmZmCVCoVfv/9d0EQBOHixYsCAOHkyZNimR07dggSiUS4c+eOIAiC8NNPPwl2dnYqdfzss88Eb29vDddIVUXJzcCBAyu8pi7VTxAEIT09XQAgHDhwQBAE9f1efvrpp0Lr1q1VXmvo0KFC3759NV0lFU/XTxBKvhyf/CJ5Wl2qXyk7Ozth+fLlBvf5lSqtnyAYzueXk5MjeHl5CVFRUSp1qgufIbulnlFhYSFOnTqF0NBQ8ZiRkRFCQ0MRExOjw8iq7+rVq2jUqBGaN2+O4cOHIykpCQBw6tQpFBUVqdTNx8cHTZs2FesWExMDf39/ODs7i2X69u2L7OxsxMfHa7ci1XDz5k2kpqaq1MnGxgadOnVSqZOtrS3at28vlgkNDYWRkRGOHz8ulunRowfMzMzEMn379kVCQgIePnyopdpULDo6Gk5OTvD29sZ7772H+/fvi+fqWv2ysrIAAPb29gDU93sZExOjco/SMtr+d/t0/Ur99ttvcHR0hJ+fH6ZMmYL8/HzxXF2qn0KhwLp165CXl4fg4GCD+/yerl8pQ/j8xo8fjwEDBpSJoy58hvVu40x1y8jIgEKhUPkAAcDZ2RmXL1/WUVTV16lTJ6xatQre3t5ISUnBzJkz0b17d8TFxSE1NRVmZmawtbVVucbZ2RmpqakAgNTU1HLrXnpO35TGVF7MT9bJyclJ5byJiQns7e1VyjRr1qzMPUrP2dnZaST+6ujXrx8GDx6MZs2a4fr16/j8888RFhaGmJgYGBsb16n6KZVKTJo0CV27doWfn5/4+ur4vayoTHZ2Nh49egQLCwtNVElFefUDgGHDhsHd3R2NGjXC+fPn8dlnnyEhIQF///13pbGXnqusjLbqd+HCBQQHB6OgoABWVlbYuHEjfH19cfbsWYP4/CqqH2AYn9+6detw+vRpnDx5ssy5uvBvkMlNPRcWFib+HBAQgE6dOsHd3R1//vmnVv7nTur32muviT/7+/sjICAALVq0QHR0NHr16qXDyGpu/PjxiIuLw+HDh3UdikZUVL933nlH/Nnf3x+urq7o1asXrl+/jhYtWmg7zFrx9vbG2bNnkZWVhQ0bNmDkyJE4cOCArsNSm4rq5+vrW+c/v+TkZEycOBFRUVEwNzfXdTi1wm6pZ+To6AhjY+Myo8TT0tLg4uKio6hqz9bWFi1btsS1a9fg4uKCwsJCZGZmqpR5sm4uLi7l1r30nL4pjamyz8vFxQXp6ekq54uLi/HgwYM6We/mzZvD0dER165dA1B36jdhwgRs27YN+/fvR5MmTcTj6vq9rKiMTCbTSmJfUf3K06lTJwBQ+Qz1vX5mZmbw9PREUFAQIiMjERgYiAULFhjM51dR/cpT1z6/U6dOIT09He3atYOJiQlMTExw4MAB/PDDDzAxMYGzs7Pef4ZMbp6RmZkZgoKCsHfvXvGYUqnE3r17Vfpf64rc3Fxcv34drq6uCAoKgqmpqUrdEhISkJSUJNYtODgYFy5cUPmyjIqKgkwmE5to9UmzZs3g4uKiUqfs7GwcP35cpU6ZmZk4deqUWGbfvn1QKpXi/6SCg4Nx8OBBFBUViWWioqLg7e2t0y6p8ty+fRv379+Hq6srAP2vnyAImDBhAjZu3Ih9+/aV6R5T1+9lcHCwyj1Ky2j6321V9SvP2bNnAUDlM9TX+lVEqVRCLpfX+c+vIqX1K09d+/x69eqFCxcu4OzZs+Kjffv2GD58uPiz3n+GzzwkmYR169YJUqlUWLVqlXDx4kXhnXfeEWxtbVVGieurjz76SIiOjhZu3rwpHDlyRAgNDRUcHR2F9PR0QRBKpvs1bdpU2LdvnxAbGysEBwcLwcHB4vWl0/369OkjnD17Vti5c6fQsGFDnU4Fz8nJEc6cOSOcOXNGACB89913wpkzZ4TExERBEEqmgtva2gqbN28Wzp8/LwwcOLDcqeBt27YVjh8/Lhw+fFjw8vJSmSqdmZkpODs7C2+88YYQFxcnrFu3TrC0tNTKVOnK6peTkyN8/PHHQkxMjHDz5k1hz549Qrt27QQvLy+hoKCgTtTvvffeE2xsbITo6GiVqbT5+fliGXX8XpZOQ/3kk0+ES5cuCT/++KNWptpWVb9r164Js2bNEmJjY4WbN28KmzdvFpo3by706NGjTtRPEARh8uTJwoEDB4SbN28K58+fFyZPnixIJBJh9+7dgiDU7c+vqvoZwudXnqdngOn7Z8jkRk0WLlwoNG3aVDAzMxM6duwoHDt2TNchVcvQoUMFV1dXwczMTGjcuLEwdOhQ4dq1a+L5R48eCePGjRPs7OwES0tLYdCgQUJKSorKPW7duiWEhYUJFhYWgqOjo/DRRx8JRUVF2q6KaP/+/QKAMo+RI0cKglAyHfw///mP4OzsLEilUqFXr15CQkKCyj3u378vRERECFZWVoJMJhNGjx4t5OTkqJQ5d+6c0K1bN0EqlQqNGzcW5syZo/P65efnC3369BEaNmwomJqaCu7u7sKYMWPKJNr6XL/y6gZAWLlypVhGXb+X+/fvF9q0aSOYmZkJzZs3V3kNXdUvKSlJ6NGjh2Bvby9IpVLB09NT+OSTT1TWSdHn+gmCILz55puCu7u7YGZmJjRs2FDo1auXmNgIQt3+/ASh8voZwudXnqeTG33/DCWCIAjP3v5DREREpB845oaIiIgMCpMbIiIiMihMboiIiMigMLkhIiIig8LkhoiIiAwKkxsiIiIyKExuiIiIyKAwuSGiesHDwwPff/+9rsMgIi1gckNEajdq1CiEh4cDAEJCQjBp0iStvfaqVatga2tb5vjJkydVdmsmIsNlousAiIiqo7CwEGZmZrW+vmHDhmqMhoj0GVtuiEhjRo0ahQMHDmDBggWQSCSQSCS4desWACAuLg5hYWGwsrKCs7Mz3njjDWRkZIjXhoSEYMKECZg0aRIcHR3Rt29fAMB3330Hf39/NGjQAG5ubhg3bhxyc3MBANHR0Rg9ejSysrLE15sxYwaAst1SSUlJGDhwIKysrCCTyfDqq68iLS1NPD9jxgy0adMGa9asgYeHB2xsbPDaa68hJydHLLNhwwb4+/vDwsICDg4OCA0NRV5enobeTSKqLiY3RKQxCxYsQHBwMMaMGYOUlBSkpKTAzc0NmZmZeP7559G2bVvExsZi586dSEtLw6uvvqpy/erVq2FmZoYjR45gyZIlAAAjIyP88MMPiI+Px+rVq7Fv3z58+umnAIAuXbrg+++/h0wmE1/v448/LhOXUqnEwIED8eDBAxw4cABRUVG4ceMGhg4dqlLu+vXr2LRpE7Zt24Zt27bhwIEDmDNnDgAgJSUFERERePPNN3Hp0iVER0dj8ODB4HZ9RLrHbiki0hgbGxuYmZnB0tISLi4u4vFFixahbdu2+Prrr8VjK1asgJubG65cuYKWLVsCALy8vDB37lyVez45fsfDwwNffvklxo4di59++glmZmawsbGBRCJReb2n7d27FxcuXMDNmzfh5uYGAPjll1/QunVrnDx5Eh06dABQkgStWrUK1tbWAIA33ngDe/fuxVdffYWUlBQUFxdj8ODBcHd3BwD4+/s/w7tFROrClhsi0rpz585h//79sLKyEh8+Pj4ASlpLSgUFBZW5ds+ePejVqxcaN24Ma2trvPHGG7h//z7y8/Or/fqXLl2Cm5ubmNgAgK+vL2xtbXHp0iXxmIeHh5jYAICrqyvS09MBAIGBgejVqxf8/f0xZMgQ/Pzzz3j48GH13wQi0hgmN0Skdbm5uXjxxRdx9uxZlcfVq1fRo0cPsVyDBg1Urrt16xZeeOEFBAQE4K+//sKpU6fw448/AigZcKxupqamKs8lEgmUSiUAwNjYGFFRUdixYwd8fX2xcOFCeHt74+bNm2qPg4hqhskNEWmUmZkZFAqFyrF27dohPj4eHh4e8PT0VHk8ndA86dSpU1AqlZg/fz46d+6Mli1b4u7du1W+3tNatWqF5ORkJCcni8cuXryIzMxM+Pr6VrtuEokEXbt2xcyZM3HmzBmYmZlh48aN1b6eiDSDyQ0RaZSHhweOHz+OW7duISMjA0qlEuPHj8eDBw8QERGBkydP4vr169i1axdGjx5daWLi6emJoqIiLFy4EDdu3MCaNWvEgcZPvl5ubi727t2LjIyMcrurQkND4e/vj+HDh+P06dM4ceIERowYgZ49e6J9+/bVqtfx48fx9ddfIzY2FklJSfj7779x7949tGrVqmZvEBGpHZMbItKojz/+GMbGxvD19UXDhg2RlJSERo0a4ciRI1AoFOjTpw/8/f0xadIk2Nrawsio4v8tBQYG4rvvvsM333wDPz8//Pbbb4iMjFQp06VLF4wdOxZDhw5Fw4YNywxIBkpaXDZv3gw7Ozv06NEDoaGhaN68Of74449q10smk+HgwYPo378/WrZsiS+++ALz589HWFhY9d8cItIIicB5i0RERGRA2HJDREREBoXJDRERERkUJjdERERkUJjcEBERkUFhckNEREQGhckNERERGRQmN0RERGRQmNwQERGRQWFyQ0RERAaFyQ0REREZFCY3REREZFCY3BAREZFB+X/6mqJU8us1fwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "sns.lineplot(x=range(LARGE_ITERS), y=loss_list)\n",
        "plt.title(\"Loss in training, 6 Transformer Blocks\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnLwPMbrc4lq"
      },
      "source": [
        "### Here I implement the variable attention model, per the meta paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "xTzrnY4-RlfU"
      },
      "outputs": [],
      "source": [
        "class MaskedHead(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size, context_window_size, embed_size=384):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          head_size: int, size of the head embedding dimension (K)\n",
        "          context_window_size: int, number of tokens considered in the past for attention (T)\n",
        "          embed_size: int, size of the token embedding dimension (D)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.head_size = head_size\n",
        "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
        "        self.query = nn.Linear(embed_size, head_size, bias=False)\n",
        "        self.value = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.z = nn.Parameter(torch.randn(1)) # learnable parameter controlling attention span\n",
        "        # I also tried making this a learnable parameter but seems the best strat is to fix at 10 as in the paper\n",
        "        self.R = 10\n",
        "        # self.R = nn.Parameter(torch.tensor(10.0))\n",
        "        # also played with l1 regularization but decided not to include for simplicity\n",
        "        self.lambda_reg = 0#1e-20\n",
        "        self.cached_mask = None\n",
        "        self.cached_T = None\n",
        "        # not a param of the model, so registered as a buffer\n",
        "        self.register_buffer('tril', torch.tril(\n",
        "            torch.ones(context_window_size, context_window_size))) # lower diagonal matrix\n",
        "\n",
        "\n",
        "\n",
        "    def compute_masking_matrix(self, z, R, T):\n",
        "        # so that we don't recompute everytime\n",
        "        if self.cached_mask is not None and self.cached_T == T and torch.allclose(self.cached_z, z):\n",
        "            return self.cached_mask\n",
        "\n",
        "        positions = torch.arange(T, device=z.device)\n",
        "        distance_matrix = positions.unsqueeze(1) - positions.unsqueeze(0)\n",
        "        distance_matrix = distance_matrix.abs()\n",
        "\n",
        "        # as given in the meta paper\n",
        "        mask = (1 / R) * (R + z - distance_matrix)\n",
        "        mask = torch.clamp(mask, min=0, max=1)\n",
        "\n",
        "        self.cached_mask = mask\n",
        "        self.cached_T = T\n",
        "        self.cached_z = z.detach().clone()\n",
        "\n",
        "        return mask  # Shape: (T, T)\n",
        "\n",
        "    def masked_attention(self, attn_scores):\n",
        "        B, T, _ = attn_scores.shape\n",
        "        mask = self.compute_masking_matrix(self.z, self.R, T).unsqueeze(0)\n",
        "        log_mask = torch.log(mask.clamp(min=1e-10))\n",
        "        masked_scores = attn_scores + log_mask #instead of multiplying just add the log before softmax\n",
        "        attn_probs = torch.softmax(masked_scores, dim=-1)\n",
        "\n",
        "        return attn_probs\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          x: (B,T,D) tensor of token embeddings\n",
        "\n",
        "        Returns:\n",
        "          (B,T,D) tensor of attention-weighted token embeddings\n",
        "        \"\"\"\n",
        "\n",
        "        attn_weights = self.query(x) @ self.key(x).transpose(-2,-1) # (B,T,T)\n",
        "        attn_weights = attn_weights.masked_fill((self.tril==0)[0:attn_weights.shape[1],0:attn_weights.shape[2]], float('-inf')) # (B,T,T)\n",
        "        masked_attn_weights = self.masked_attention(attn_weights)\n",
        "        avg_embeddings = torch.einsum('bij,bjd->bid', masked_attn_weights, self.value(x))\n",
        "        l1_penalty = self.lambda_reg * self.z.abs().sum()\n",
        "\n",
        "        return avg_embeddings,l1_penalty\n",
        "\n",
        "\n",
        "class MultiHeadAttention_Masked(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, context_window_size, num_heads, head_size, embed_size=384):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            context_window_size: int, number of tokens considered in the past for attention (T)\n",
        "            num_heads: int, number of heads (H)\n",
        "            head_size: int, size of the head embedding dimension\n",
        "            embed_size: int, size of the token embedding dimension\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # TODO, your code below\n",
        "        # had to change to module list to play nice with l1 penalty\n",
        "        self.heads = nn.ModuleList([MaskedHead(head_size, context_window_size, embed_size) for _ in range(num_heads)])\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "\n",
        "    def forward(self, x): # x is (B,T,D)\n",
        "        # TODO, your code below\n",
        "        heads = [head(x) for head in self.heads] # list of (B,T,D)\n",
        "        avg_embeddings = torch.stack([h[0] for h in heads]).sum(dim=0) # (B,T,D) - just sum across heads elementwise for each token\n",
        "        total_l1_penalty = sum(h[1] for h in heads) / self.num_heads  # Average L1 across heads\n",
        "        return avg_embeddings, total_l1_penalty\n",
        "        # pass\n",
        "class TransformerBlock_VarAttn(nn.Module):\n",
        "    \"\"\" Transformer block: communication across sequence length, followed by communication across embedding space\n",
        "        Uses multi-headed attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6,dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(embed_size)\n",
        "        self.ln2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        # TODO: your code below\n",
        "        self.feed_forward = FeedForward(embed_size) # acts along rows x_t^(m+1) = mlp(y_t^(m))\n",
        "        self.atten_heads = MultiHeadAttention_Masked(context_window_size, num_heads, embed_size//num_heads, embed_size) # as before\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "        # for dropout\n",
        "        self.dropout = nn.Dropout(dropout) # takes a dropout rate arg\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output, l1_penalty = self.atten_heads(self.ln1(x))\n",
        "        x = x + self.dropout(attn_output)\n",
        "        x = x + self.dropout(self.feed_forward(self.ln2(x)))\n",
        "        return x, l1_penalty\n",
        "\n",
        "class TransformerLM_VarAttn(nn.Module):\n",
        "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6, n_layers=6):\n",
        "        \"\"\"\n",
        "          Args:\n",
        "              vocab_size: int, number of tokens in the vocabulary (V)\n",
        "              context_window_size: int, size of the context window (T)\n",
        "              embed_size: int, embedding size (D)\n",
        "              num_heads: int, number of heads (H)\n",
        "              n_layers: int, number of layers (M)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
        "        self.position_embedding_table = nn.Embedding(context_window_size, embed_size)\n",
        "        # module list again bc l1_penalty\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock_VarAttn(vocab_size, context_window_size, embed_size=embed_size, num_heads=num_heads)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(embed_size)\n",
        "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Good initialization\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, token_ids, targets=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            token_ids: (B, T) tensor of integers, provides the context\n",
        "            targets: (B, T) tensor of integers, provides the tokens we are predicting\n",
        "        \"\"\"\n",
        "        B, T = token_ids.shape\n",
        "        tok_emb = self.token_embedding_table(token_ids)  # (B, T, D)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=token_ids.device))  # (T, D)\n",
        "        x = tok_emb + pos_emb  # (B, T, D)\n",
        "\n",
        "        total_l1_penalty = 0\n",
        "        # does the sequential processing manually since using module list instead of sequential\n",
        "        for block in self.blocks:\n",
        "            x, l1_penalty = block(x)\n",
        "            total_l1_penalty += l1_penalty\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss_fcn = torch.nn.CrossEntropyLoss()\n",
        "        loss = loss_fcn(logits.reshape(-1, self.vocab_size), targets.reshape(-1)) + total_l1_penalty\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, token_ids, max_new_tokens):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            token_ids: (B, T) tensor of integers forming the context.\n",
        "            max_new_tokens: int, max number of tokens to generate.\n",
        "        \"\"\"\n",
        "        for i in range(max_new_tokens):\n",
        "            B, T = token_ids.shape  # (B, T)\n",
        "            t = min(T, self.position_embedding_table.num_embeddings)\n",
        "            tok_emb = self.token_embedding_table(token_ids[:, -t:]).reshape(B, t, -1)  # (B, T, D)\n",
        "            pos_emb = self.position_embedding_table(torch.arange(t, device=token_ids.device))  # (T, D)\n",
        "            x = tok_emb + pos_emb  # (B, T, D)\n",
        "            for block in self.blocks:\n",
        "                x, _ = block(x)\n",
        "            x = self.ln_f(x)\n",
        "            logits = self.lm_head(x)  # (B, T, V)\n",
        "            new_token = torch.distributions.Categorical(logits=logits[:, -1]).sample()\n",
        "            token_ids = torch.cat([token_ids, new_token.unsqueeze(1)], dim=1)\n",
        "        return token_ids\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkZccvv4c4lr"
      },
      "source": [
        "### this code block updates train data according to how many bi and trigrams we include"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZAQzEQ5Z5HK",
        "outputId": "5d5d1406-4bc9-4944-dc0d-e5f37fa8f8e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 664,015 tokens\n",
            "val has 75,083 tokens\n",
            "Dictionary has 315 tokens\n"
          ]
        }
      ],
      "source": [
        "# find the most common bigrams and trigrams (doubles and triple character combination)\n",
        "from collections import Counter\n",
        "#update this\n",
        "n_biandtri = 250\n",
        "\n",
        "# Ensure 'data' is defined with your full text\n",
        "n = len(data)\n",
        "train_chars = data[:int(n * 0.9)]\n",
        "val_chars = data[int(n * 0.9):]\n",
        "\n",
        "def BiandTrigrams(text, top=10):\n",
        "\n",
        "    text = text.replace(\"\\n\", \"\").replace(\" \", \"\")  # Remove spaces and newlines\n",
        "\n",
        "    ngram_count = Counter()\n",
        "    #pdb.st_trace()\n",
        "    for size in [2,3]:\n",
        "        ngrams = [text[i:i+size] for i in range(len(text) - size + 1)]\n",
        "        ngram_count.update(ngrams)\n",
        "\n",
        "    return [ngram for ngram, _ in ngram_count.most_common(top)]\n",
        "\n",
        "\n",
        "\n",
        "out = BiandTrigrams(train_chars, n_biandtri)\n",
        "#top 100 bigrams and trigrams\n",
        "\n",
        "# get all the unique characters that occur in this text\n",
        "chars = sorted(list(set(data)))\n",
        "\n",
        "dic = chars+list(out)\n",
        "\n",
        "# create a mapping from dic tionary of integers and bi/trigrams to integers\n",
        "stoi2 = { ch:i for i,ch in enumerate(dic) }\n",
        "itos2 = { i:ch for i,ch in enumerate(dic) }\n",
        "\n",
        "def encodeBiandTri(text):\n",
        "\n",
        "    i = 0\n",
        "    encoded_string = []\n",
        "    while i < len(text):\n",
        "\n",
        "        if i + 2 < len(text) and text[i:i+3] in stoi2:\n",
        "            encoded_string.append(stoi2[text[i:i+3]])\n",
        "            i += 3 #checks next three integers\n",
        "\n",
        "        elif i + 1 < len(text) and text[i:i+2] in stoi2:\n",
        "            encoded_string.append(stoi2[text[i:i+2]])\n",
        "            i += 2\n",
        "        # As above with doubles\n",
        "        elif text[i] in stoi2:\n",
        "            encoded_string.append(stoi2[text[i]])\n",
        "            i += 1\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown token at index {i}: {text[i]}\")\n",
        "    return encoded_string\n",
        "\n",
        "def decode2(l):\n",
        "    return ''.join([itos2[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "\n",
        "# Encode both datasets into lists of integers\n",
        "train_data = encodeBiandTri(train_chars)\n",
        "val_data = encodeBiandTri(val_chars)\n",
        "\n",
        "# Cast the lists to torch tensors\n",
        "train_data = torch.tensor(train_data)\n",
        "val_data = torch.tensor(val_data)\n",
        "\n",
        "print(f\"train has {len(train_data):,} tokens\")\n",
        "print(f\"val has {len(val_data):,} tokens\")\n",
        "\n",
        "vocab_size=len(dic)\n",
        "print(f\"Dictionary has {vocab_size} tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "5zN24pfkYyAT"
      },
      "outputs": [],
      "source": [
        "# function for getting batches of data\n",
        "def get_batch(split, context_window_size, device, batch_size=32):\n",
        "    \"\"\"\n",
        "    generate a small batch of data of inputs x and targets y\n",
        "\n",
        "    Args:\n",
        "        split: 'train' or 'val'\n",
        "        device: 'cpu' or 'cuda' (should be 'cuda' if available)\n",
        "    \"\"\"\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - context_window_size, (batch_size,))  # generate a bunch of starting positions\n",
        "    x = torch.stack([data[i:i+context_window_size] for i in ix]) # stack the data from each starting point\n",
        "    y = torch.stack([data[i+1:i+context_window_size+1] for i in ix])\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# helper function for tracking loss during training\n",
        "# given to you\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model, eval_iters, context_window_size, device):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      model: model being evaluated\n",
        "      eval_iters: number of batches to average over\n",
        "      context_window_size: size of the context window\n",
        "      device: 'cpu' or 'cuda' (should be 'cuda' if available)\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split, context_window_size, device)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNIK3j0F82d8",
        "outputId": "c23e07c5-5589-4339-905a-b074384e59da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-39-ce865b916456>:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "  0%|          | 0/4000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-39-ce865b916456>:63: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "\r  0%|          | 1/4000 [00:24<26:48:34, 24.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 5.7527, val loss 5.7541\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 200/4000 [00:52<08:49,  7.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 200 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 201/4000 [01:16<7:49:11,  7.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: train loss 3.4808, val loss 3.4641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 400/4000 [01:44<08:20,  7.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 400 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 401/4000 [02:08<7:27:52,  7.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: train loss 3.1366, val loss 3.1743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▌        | 600/4000 [02:36<08:01,  7.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 600 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 601/4000 [03:01<7:00:11,  7.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: train loss 2.9752, val loss 3.0501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 800/4000 [03:28<07:27,  7.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 800 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 801/4000 [03:53<6:38:10,  7.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: train loss 2.8234, val loss 2.9492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 1000/4000 [04:21<06:57,  7.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1000 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 1001/4000 [04:45<6:10:34,  7.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1000: train loss 2.7162, val loss 2.8757\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 1200/4000 [05:13<06:34,  7.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1200 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 1201/4000 [05:37<5:46:01,  7.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1200: train loss 2.6034, val loss 2.8117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|███▌      | 1400/4000 [06:05<06:03,  7.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1400 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▌      | 1401/4000 [06:30<5:23:13,  7.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1400: train loss 2.4934, val loss 2.7155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 1600/4000 [06:58<05:32,  7.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1600 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 1601/4000 [07:22<4:56:26,  7.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1600: train loss 2.3883, val loss 2.6296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▌     | 1800/4000 [07:50<05:06,  7.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1800 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 1801/4000 [08:14<4:31:43,  7.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1800: train loss 2.3084, val loss 2.5656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 2000/4000 [08:42<04:39,  7.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 2000 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 2001/4000 [09:07<4:06:57,  7.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 2000: train loss 2.2386, val loss 2.5327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|█████▌    | 2200/4000 [09:34<04:10,  7.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 2200 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▌    | 2201/4000 [09:59<3:42:10,  7.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 2200: train loss 2.1632, val loss 2.4804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 2400/4000 [10:27<03:43,  7.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 2400 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 2401/4000 [10:51<3:17:52,  7.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 2400: train loss 2.1191, val loss 2.4443\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▌   | 2600/4000 [11:19<03:16,  7.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 2600 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 2601/4000 [11:43<2:53:02,  7.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 2600: train loss 2.0653, val loss 2.4150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 2800/4000 [12:11<02:46,  7.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 2800 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 2801/4000 [12:36<2:28:17,  7.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 2800: train loss 2.0169, val loss 2.3961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 3000/4000 [13:04<02:19,  7.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 3000 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 3001/4000 [13:28<2:03:41,  7.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 3000: train loss 1.9786, val loss 2.3765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 3200/4000 [13:56<01:53,  7.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 3200 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 3201/4000 [14:20<1:38:46,  7.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 3200: train loss 1.9293, val loss 2.3757\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 85%|████████▌ | 3400/4000 [14:48<01:23,  7.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 3400 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▌ | 3401/4000 [15:13<1:14:07,  7.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 3400: train loss 1.8867, val loss 2.3448\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 3600/4000 [15:40<00:55,  7.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 3600 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 3601/4000 [16:05<49:19,  7.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 3600: train loss 1.8560, val loss 2.3540\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▌| 3800/4000 [16:33<00:28,  6.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 3800 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▌| 3801/4000 [16:57<24:38,  7.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 3800: train loss 1.8044, val loss 2.3675\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 3999/4000 [17:25<00:00,  7.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 3999 batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4000/4000 [17:49<00:00,  3.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 3999: train loss 1.7778, val loss 2.3696\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "LARGE_ITERS = 4000\n",
        "import itertools\n",
        "\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "scaler = GradScaler()\n",
        "\n",
        "# trans = TransformerLM_VarAttn(vocab_size, CONTEXT_WINDOW_SIZE)\n",
        "trans = TransformerLM_2(vocab_size, CONTEXT_WINDOW_SIZE)\n",
        "tlm = trans.to(device)\n",
        "learning_rate = 1e-4\n",
        "# TODO, your code below\n",
        "\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "torch.manual_seed(760)\n",
        "\n",
        "optimizer = torch.optim.AdamW(trans.parameters(), lr=learning_rate)\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': [p for n, p in trans.named_parameters() if ('z' not in n and 'R' not in n)], 'lr': 1e-4},  # Default LR\n",
        "    {'params': [p for n, p in trans.named_parameters() if ('z' in n or 'R' in n)], 'lr': 0.01}  # Higher LR for z\n",
        "])\n",
        "# scheduler = CosineAnnealingLR(optimizer, T_max=LARGE_ITERS, eta_min=1e-6)\n",
        "\n",
        "eval_interval = 200\n",
        "\n",
        "# batch params\n",
        "batch_start = 16\n",
        "batch_max = 128\n",
        "batch_growth = LARGE_ITERS//4\n",
        "\n",
        "\n",
        "loss_list = []\n",
        "min_prev_val_loss = torch.tensor([100.]) #init\n",
        "for it in tqdm(range(LARGE_ITERS)):\n",
        "    #update the batch size\n",
        "    # batch_size = min(batch_start *(2**(it // batch_growth)), batch_max)\n",
        "    batch_size = 32\n",
        "\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if it % eval_interval == 0 or it == LARGE_ITERS - 1:\n",
        "        print(f\"iteration {it}\", f\"batch size {batch_size}\")\n",
        "        losses = estimate_loss(tlm, EVAL_ITERS, CONTEXT_WINDOW_SIZE, device)\n",
        "\n",
        "        print(f\"step {it}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        # update minimum prev val loss\n",
        "        min_prev_val_loss = torch.min(losses['val'],min_prev_val_loss)\n",
        "\n",
        "\n",
        "        # zs = list(itertools.chain.from_iterable( [[10 + head.z.item() +1 for head in tlm.blocks[i].atten_heads.heads] for i in range(6)]))\n",
        "        # Rs = list(itertools.chain.from_iterable( [[head.R  for head in tlm.blocks[i].atten_heads.heads] for i in range(6)]))\n",
        "\n",
        "\n",
        "        # print(f'min z: {min(zs)}, max; {max(zs)}, min R {min(Rs)}, max R {max(Rs)}')\n",
        "        # stop once our losses['val'] starts increasing\n",
        "        if it > 0 and losses['val'] > 1.02*min_prev_val_loss:\n",
        "            break\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train', CONTEXT_WINDOW_SIZE, device, batch_size=batch_size)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # evaluate the loss\n",
        "    with autocast():\n",
        "      logits, loss = tlm(xb, yb)\n",
        "    scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "    # for head in tlm.blocks[0].atten_heads.heads:\n",
        "    #     print(10 + head.z.item() +1)\n",
        "    # messed around w grad clipping but decided not to include\n",
        "    # torch.nn.utils.clip_grad_norm_(tlm.parameters(), 1)\n",
        "    loss_list.append(loss.detach().item())\n",
        "\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # scheduler.step()\n",
        "# torch.save({\"model\": trans, \"loss_list\": loss_list}, \"RegAttn_250_model.pth\")\n",
        "# files.download(\"RegAttn_250_model.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "z5WmzWqZW1pl"
      },
      "outputs": [],
      "source": [
        "# torch.save({\"model\": trans, \"loss_list\": loss_list}, \"RegAttn_250_model.pth\")\n",
        "# files.download(\"RegAttn_250_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "5Kz6hhZV4ccS"
      },
      "outputs": [],
      "source": [
        "# # sum the zs over all heads\n",
        "# import numpy as np\n",
        "# zs = list(itertools.chain.from_iterable( [[head.z.item()for head in tlm.blocks[i].atten_heads.heads] for i in range(6)]))\n",
        "# 1e-20 * (np.abs(np.array(zs)).sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "02Vfnx96vIgz"
      },
      "outputs": [],
      "source": [
        "# # unconditional generation from the model\n",
        "# start_context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "# uncond_gen = (tlm.generate(start_context, max_new_tokens=CONTEXT_WINDOW_SIZE)[0].tolist())\n",
        "# print(decode(uncond_gen))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iN0R1ChffesX",
        "outputId": "a597a926-d987-4fbe-dc83-a239eaa1c4b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "May thrust son; and my poor emain;\n",
            "But he confided out thy rial, his holy shast\n",
            "Faincry out thy daughter's blood!\n",
            "With at you shall make the enemies aside on\n",
            "But then our son true hast\n",
            "With that I hate fear'd in hear and wooers.\n",
            "\n",
            "WARWICK:\n",
            "A mine, if true this trempets last he call'd;\n",
            "And I know he should fid the horse of him thinks\n",
            "With should be any equal the father off man\n",
            "Frest my so house! The \n"
          ]
        }
      ],
      "source": [
        "# unconditional generation from the model\n",
        "start_context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "uncond_gen = (tlm.generate(start_context, max_new_tokens=CONTEXT_WINDOW_SIZE)[0].tolist())\n",
        "print(decode2(uncond_gen))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mefVVuDDnXsN"
      },
      "outputs": [],
      "source": [
        "        # print([10 + head.z.item() +1 for head in tlm.blocks[0].atten_heads.heads])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQRIK5-Bm5GY"
      },
      "outputs": [],
      "source": [
        "# tlm.blocks[1].atten_heads.heads[4].z.grad # making sure that the zs are getting gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3ZhuSPln2zH"
      },
      "outputs": [],
      "source": [
        "# for head in tlm.blocks[0].atten_heads.heads:\n",
        "#     print(f\"z grad: {head.z.grad}, requires_grad: {head.z.requires_grad}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3Lj3GrBi6Oa"
      },
      "outputs": [],
      "source": [
        "# tlm.to('cpu')\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4QbO4BlA29V"
      },
      "outputs": [],
      "source": [
        "# # unconditional generation from the model\n",
        "# start_context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "# uncond_gen = (tlm.generate(start_context, max_new_tokens=CONTEXT_WINDOW_SIZE)[0].tolist())\n",
        "# print(decode(uncond_gen))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUwAzNUfsAvP"
      },
      "outputs": [],
      "source": [
        "\n",
        "# for head in tlm.blocks[0].atten_heads.heads:\n",
        "#     print(10 + head.z.item() +1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QG1jISFjzKr"
      },
      "outputs": [],
      "source": [
        "# vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLN_ev93c4lt"
      },
      "source": [
        "### some scripts here to load our models to calculate the train and test loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRSPWRvOhZjp"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "\n",
        "# model_dir = '/content/'\n",
        "\n",
        "# for filename in os.listdir(model_dir):\n",
        "#     if filename.endswith('.pth'):\n",
        "#       if '_250_' in filename:\n",
        "#         # break\n",
        "\n",
        "#           model_path = os.path.join(model_dir, filename)\n",
        "#           tlm = torch.load(model_path)\n",
        "#           tlm = tlm['model']\n",
        "\n",
        "#           losses = estimate_loss(tlm, EVAL_ITERS, CONTEXT_WINDOW_SIZE, device)\n",
        "#           print(f\"Model: {filename}, train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idxGCeIFc4lt"
      },
      "outputs": [],
      "source": [
        "# z_values.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "781gZ3sTc4lt"
      },
      "source": [
        "# plotting for the writeup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yI886Chc4lt"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import torch\n",
        "\n",
        "# # load all the pth files in downloads and concatenate the z values for each head in each block to an array, with rows being each model and columns being each head\n",
        "\n",
        "# model_dir = '/Users/colecitrenbaum/Downloads/'  # Adjust this path to your model directory\n",
        "# z_values = []\n",
        "# model_names = []\n",
        "# for filename in os.listdir(model_dir):\n",
        "#     if filename.endswith('.pth'):\n",
        "#         if 'Var' in filename:\n",
        "#             model_path = os.path.join(model_dir, filename)\n",
        "#             filename = filename.replace('_model.pth','')\n",
        "#             filename = filename.replace('VarAttn_', '')\n",
        "#             model_names.append(filename)\n",
        "#             model_data = torch.load(model_path,weights_only=False,map_location=torch.device('cpu'))\n",
        "#             model = model_data['model']\n",
        "\n",
        "#             zs = []\n",
        "#             for block in model.blocks:\n",
        "#                 for head in block.atten_heads.heads:\n",
        "#                     zs.append(head.z.item())\n",
        "#             z_values.append(zs)\n",
        "\n",
        "\n",
        "# z_values = torch.tensor(z_values)\n",
        "# z_values = z_values + 1 +10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QnW_Y4Hc4lt"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# zs = pd.DataFrame(z_values).T\n",
        "# zs.columns = model_names\n",
        "# # reorder columns of zs\n",
        "# zs = zs[['CharsOnly','50','100','250']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28KR-MVGc4lt"
      },
      "outputs": [],
      "source": [
        "# fig, ax = plt.subplots(figsize=(12, 3))\n",
        "# settings = ['CharsOnly', '50', '100', '250']\n",
        "# for i, setting in enumerate(settings):\n",
        "#     ax.boxplot(zs.melt(id_vars=None, value_vars=model_names).query(f\"variable == '{setting}'\").value.values,\n",
        "#                positions=[i], widths=0.4)\n",
        "# ax.set_xticks(range(len(settings)))\n",
        "# ax.set_xticklabels(settings)\n",
        "# plt.ylabel('Adaptive Attention Span ')\n",
        "# plt.xlabel('Vocabulary Size (In Addition to 56 Characters)')\n",
        "# plt.title('Learned Adaptive Attention Span', fontweight=\"bold\", fontsize=20)\n",
        "# # plt.show()\n",
        "# ax.spines['bottom'].set_color('0')\n",
        "# ax.spines['top'].set_color('0')\n",
        "# ax.spines['right'].set_color('0')\n",
        "# ax.spines['left'].set_color('0')\n",
        "# # plt.savefig('/Users/colecitrenbaum/Documents/VarAttn.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk7fvj8Cc4lt"
      },
      "outputs": [],
      "source": [
        "# zs.melt(id_vars=None, value_vars=model_names).query(f\"variable == '{setting}'\").value.values"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}